/**
 * Embedded templates for compiled binaries.
 * This file is auto-generated during build - DO NOT EDIT MANUALLY.
 * Generated at: development-build
 */

// Template contents embedded at build time
const EMBEDDED_TEMPLATES: Record<string, string> = {
  "CLAUDE.md.template": "<!-- PROMPTCODE-CLI-START -->\n# PromptCode CLI\n\nAI-ready code analysis via presets and expert consultations.\n\n## Commands\n- `/promptcode-preset-list` - List available presets\n- `/promptcode-preset-info <name>` - Show preset details & tokens\n- `/promptcode-preset-create <description>` - Create preset from description\n- `/promptcode-preset-to-prompt <preset>` - Export preset to file\n- `/promptcode-ask-expert <question>` - AI consultation with code context\n\n## Workflow Examples\n\n### Discovery â†’ Context â†’ Expert\n```bash\n/promptcode-preset-list                    # Find existing presets\n/promptcode-preset-create auth system      # Or create focused preset\n/promptcode-ask-expert Why is login slow?  # Consult with context\n```\n\n### Direct CLI Usage\n```bash\npromptcode expert \"Review this\" --preset api --yes   # After cost approval\npromptcode generate -f \"src/**/*.ts\" -o prompt.txt   # Export for external use\n```\n\n## Cost Approval Protocol\n1. CLI estimates cost (threshold: $0.50)\n2. CC asks user ONCE for approval\n3. CC re-runs with `--yes` flag\n\n## API Keys Required\n```bash\nexport OPENAI_API_KEY=sk-...     # GPT/O3 models\nexport ANTHROPIC_API_KEY=sk-...  # Claude models\nexport GOOGLE_API_KEY=...        # Gemini models\nexport XAI_API_KEY=...           # Grok models\n```\n\nðŸ’¡ **Tip**: Create focused presets for better context and lower costs.\n<!-- PROMPTCODE-CLI-END -->",
  "promptcode-ask-expert.md": "---\nallowed-tools: Task, Read(/tmp/*), Read(/var/*), Write(/tmp/*), Write(/var/*), Edit(/tmp/*), Edit(/var/*), Edit(.promptcode/presets/*), Bash, Bash(*)\ndescription: Consult AI expert for complex problems with code context - supports ensemble mode for multiple models\n---\n\nConsult an expert about: $ARGUMENTS\n\n## Instructions:\n\n1. Analyze the request in $ARGUMENTS:\n   - Extract the main question/problem\n   - Identify if code context would help (look for keywords about implementation, feature, code review, etc.)\n   - Check for multiple model requests (e.g., \"compare using gpt-5 and opus-4\", \"ask gpt-5, sonnet-4, and gemini\")\n   - Get available models dynamically: `promptcode expert --models --json` (parse the JSON for model list)\n   - If 2+ models detected â†’ use ensemble mode\n   - For single model: Use gpt-5 (the default) unless user explicitly specifies another model\n\n2. Determine code context needs:\n   ```bash\n   promptcode preset list\n   ```\n   - Check if an existing preset matches the request (e.g., \"security\" â†’ look for security-related presets)\n   - If no suitable preset exists, create one:\n     ```bash\n     promptcode preset create {descriptive-name}\n     ```\n     Then edit `.promptcode/presets/{descriptive-name}.patterns` to add relevant file patterns.\n     Or use `--from-files` with specific patterns:\n     ```bash\n     promptcode preset create {descriptive-name} --from-files \"src/**/*.ts\" \"tests/**/*.test.ts\"\n     ```\n   - Verify the preset:\n     ```bash\n     promptcode preset info {preset-name}\n     ```\n\n3. Prepare consultation file for review:\n   - Set temp directory: `TMP=\"${TMPDIR:-/tmp}\"`\n   - Create unique files: `PROMPT_FILE=\"${TMP%/}/expert-consultation-$(date +%Y%m%d-%H%M%S)-$$.txt\"`\n   - Structure the file with:\n     ```markdown\n     # Expert Consultation\n     \n     ## Question\n     {user's question}\n     \n     ## Context\n     {any relevant context or background}\n     \n     ## Code Context\n     ```\n   - Append the code context using the preset:\n     ```bash\n     CODE_FILE=\"${TMP%/}/code-context-$(date +%Y%m%d-%H%M%S)-$$.txt\"\n     promptcode generate --preset \"{preset_name}\" --output \"$CODE_FILE\"\n     cat \"$CODE_FILE\" >> \"$PROMPT_FILE\"\n     ```\n\n4. Open consultation for user review:\n   ```bash\n   # Try cursor first, then code, then EDITOR, then xdg-open/open\n   if command -v cursor &> /dev/null; then\n     cursor \"$PROMPT_FILE\"\n   elif command -v code &> /dev/null; then\n     code \"$PROMPT_FILE\"\n   elif [ -n \"$EDITOR\" ]; then\n     \"$EDITOR\" \"$PROMPT_FILE\"\n   elif command -v xdg-open &> /dev/null; then\n     xdg-open \"$PROMPT_FILE\"\n   elif command -v open &> /dev/null; then\n     open \"$PROMPT_FILE\"\n   else\n     echo \"ðŸ“„ Consultation file created at: $PROMPT_FILE\"\n     echo \"No editor found. Please open the file manually to review.\"\n   fi\n   ```\n   \n5. Estimate cost and get approval:\n   ```bash\n   promptcode expert --prompt-file \"$PROMPT_FILE\" --model <model> --estimate-cost --json\n   ```\n   - Parse JSON for `cost.total` and `tokens.input`\n   - Exit code: 0 = success, 2 = approval required (cost > threshold)\n   \n   **For single model:**\n   - Say: \"Ready to consult {model} using preset '{preset_name}' ({tokens} tokens, ~${cost}). Reply 'yes' to proceed.\"\n   \n   **For ensemble mode (multiple models):**\n   - Run --estimate-cost for each model in parallel\n   - Say: \"Ready for ensemble consultation with {models} ({total_tokens} tokens). Total: ${total_cost} ({model1}: ${cost1}, {model2}: ${cost2}). Reply 'yes' to proceed.\"\n   \n   **Important: Ask for approval ONLY ONCE - after showing cost estimate**\n\n6. Execute based on mode:\n\n   **Single Model Mode:**\n   ```bash\n   promptcode expert --prompt-file \"$PROMPT_FILE\" --model {model} --yes\n   ```\n   \n   **Ensemble Mode (Parallel Execution):**\n   - Use a SINGLE parent Task that orchestrates parallel sub-tasks (idiomatic for Claude Code)\n   - The parent Task:\n     1. Launches parallel sub-tasks for each model\n     2. Waits for all sub-tasks to complete\n     3. Reads all response files\n     4. Creates the synthesis report (Step 7)\n   - Structure:\n     ```\n     Task: \"Ensemble consultation with {model1} and {model2}\"\n     Prompt: \"\n       Step 1: Run these consultations in PARALLEL as sub-tasks:\n       - Sub-task 1: promptcode expert --prompt-file '$PROMPT_FILE' --model {model1} --yes --output /tmp/expert-{model1}.txt\n       - Sub-task 2: promptcode expert --prompt-file '$PROMPT_FILE' --model {model2} --yes --output /tmp/expert-{model2}.txt\n       \n       Step 2: After both complete, read the response files\n       Step 3: Create synthesis report as described in Step 7\n       Step 4: Report back with synthesis and winner\n     \"\n     ```\n   - Note: The --yes flag confirms we have user approval for the cost\n   - The allowed-tools configuration permits these commands to run without additional prompts\n\n7. Handle the response:\n\n   **Single Model Mode:**\n   - If successful: Open response in Cursor (if available) and summarize key insights\n   - If API key missing: Show appropriate setup instructions\n   \n   **Ensemble Mode (Synthesis):**\n   - Read all response text files\n   - Extract key insights from each model's response\n   - Create synthesis report: `SYNTHESIS_FILE=\"${TMP%/}/expert-synthesis-$(date +%Y%m%d-%H%M%S)-$$.txt\"`\n   \n   ```markdown\n   # Ensemble Expert Consultation Results\n   \n   ## Question\n   {original_question}\n   \n   ## Expert Responses\n   \n   ### {Model1} - ${actual_cost}, {response_time}s\n   **Key Points:**\n   - {key_point_1}\n   - {key_point_2}\n   - {key_point_3}\n   \n   ### {Model2} - ${actual_cost}, {response_time}s\n   **Key Points:**\n   - {key_point_1}\n   - {key_point_2}\n   - {key_point_3}\n   \n   ## Synthesis\n   \n   **Consensus Points:**\n   - {point_agreed_by_multiple_models}\n   - {another_consensus_point}\n   \n   **Best Comprehensive Answer:** {Model} provided the most thorough analysis, particularly strong on {specific_aspect}\n   \n   **Unique Insights:**\n   - {Model1}: {unique_insight_from_model1}\n   - {Model2}: {unique_insight_from_model2}\n   \n   **ðŸ† WINNER:** {winning_model} - {clear_reason_why_this_model_won}\n   (If tie: \"TIE - Both models provided equally valuable but complementary insights\")\n   \n   **Performance Summary:**\n   - Total Cost: ${total_actual_cost}\n   - Total Time: {total_time}s\n   - Best Value: {model_with_best_cost_to_quality_ratio}\n   ```\n   \n   - Open synthesis in Cursor if available\n   - IMPORTANT: Always declare a clear winner (or explicitly state if it's a tie)\n   - Provide brief summary of which model performed best and why they won\n\n   **Error Handling:**\n   - If any model fails in ensemble mode, continue with successful ones\n   - Report which models succeeded/failed\n   - If OPENAI_API_KEY missing:\n     ```\n     To use expert consultation, set your OpenAI API key:\n     export OPENAI_API_KEY=sk-...\n     Get your key from: https://platform.openai.com/api-keys\n     ```\n   - For other errors: Report exact error message\n\n## Important:\n- **Always use presets** - either existing or create new ones for code context\n- **Single approval flow**: Estimate cost â†’ Ask user ONCE â†’ Execute with --yes\n- **Show the preset name** to the user so they know what context is being used\n- **Default model is gpt-5** - use this unless user explicitly requests another model\n- Discover default model via `promptcode expert --models --json` (look for `defaultModel: \"gpt-5\"`)\n- For ensemble mode: limit to maximum 4 models\n- NEVER automatically add --yes without user approval\n- Reasoning effort defaults to 'high' (set in CLI) - no need to specify\n- Always use `--output` flag instead of stdout redirection for reliability",
  "promptcode-preset-create.md": "---\nallowed-tools: Bash(promptcode preset create:*), Bash(promptcode preset info:*), Glob(**/*), Grep, Write(.promptcode/presets/*.patterns)\ndescription: Create a promptcode preset from description\n---\n\nCreate a promptcode preset for: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the description to understand what code to capture:\n   - Look for keywords like package names, features, components, integrations\n   - Identify if it's Python, TypeScript, or mixed code\n   - Determine the scope (single package, cross-package feature, etc.)\n\n2. Research the codebase structure:\n   - Use Glob to explore relevant directories\n   - Use Grep to find related files if needed\n   - Identify the main code locations and any related tests/docs\n\n3. Generate a descriptive preset name:\n   - Use kebab-case (e.g., \"auth-system\", \"microlearning-utils\")\n   - Keep it concise but descriptive\n\n4. Create the preset (automatically optimized from concrete files):\n   ```bash\n   # When you identify specific files, always use --from-files for smart optimization\n   promptcode preset create \"{preset_name}\" --from-files {file-globs...}\n   # default optimization-level is \"balanced\"\n   # to control: --optimization-level minimal|balanced|aggressive\n   ```\n   This creates `.promptcode/presets/{preset_name}.patterns` with optimized patterns.\n\n5. Edit the preset file to add patterns (if needed):\n   - Start with a header comment explaining what the preset captures\n   - Add inclusion patterns for the main code\n   - Add patterns for related tests and documentation\n   - Include common exclusion patterns:\n     - `!**/__pycache__/**`\n     - `!**/*.pyc`\n     - `!**/node_modules/**`\n     - `!**/dist/**`\n     - `!**/build/**`\n\n6. Test and report results:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n   Report the file count and estimated tokens.\n\n## Common Pattern Examples:\n- Python package: `python/cogflows-py/packages/{package}/src/**/*.py`\n- TypeScript component: `ts/next/{site}/components/{component}/**/*.{ts,tsx}`\n- Cross-package feature: Multiple specific paths\n- Tests: `python/cogflows-py/packages/{package}/tests/**/*.py`\n- Documentation: `**/{feature}/**/*.md`",
  "promptcode-preset-info.md": "---\nallowed-tools: Bash(promptcode preset info:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*)\ndescription: Show detailed information about a promptcode preset\n---\n\nShow detailed information about promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the arguments to identify the preset:\n   - If exact preset name provided (e.g., \"functional-framework\"), use it directly\n   - If description provided, infer the best matching preset:\n     - Run `promptcode preset list` to see available presets\n     - Read header comments from preset files in `.promptcode/presets/` if needed\n     - Match based on keywords and context\n     - Choose the most relevant preset\n\n2. Run the promptcode info command with the determined preset name:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n\n3. If a preset was inferred from description, explain which preset was chosen and why.\n\nThe output will show:\n- Preset name and path\n- Description from header comments\n- File count and token statistics\n- Pattern details\n- Sample files included\n- Usage instructions",
  "promptcode-preset-list.md": "---\nallowed-tools: Bash(promptcode preset list:*)\ndescription: List all available promptcode presets with pattern counts\n---\n\nList all available promptcode presets.\n\nRun the command:\n```bash\npromptcode preset list\n```\n\nThis will display all available presets with their pattern counts. Use the preset names with other promptcode commands to work with specific code contexts.",
  "promptcode-preset-to-prompt.md": "---\nallowed-tools: Bash(promptcode generate:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*)\ndescription: Generate AI-ready prompt file from a promptcode preset with optional instructions\n---\n\nGenerate prompt file from promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse arguments to understand what the user wants:\n   - Extract preset name or description (may be quoted)\n   - Detect explicit instructions delimiter:\n     - If arguments contain \" -- \" (space, two dashes, space), everything after it is the instructions (preserve verbatim)\n     - Else, look for explicit -i or --instructions value; if provided, use it verbatim as instructions\n   - Keep remaining tokens for output path and/or fallback instructions\n\n2. If inferring from description:\n   - Run `promptcode preset list` to see available presets\n   - Read header comments from `.promptcode/presets/*.patterns` files if needed\n   - Match based on keywords and context\n   - Choose the most relevant preset\n\n3. Determine output path and instructions (fallback):\n   - Resolve output path using existing keywords:\n     - \"to [path]\" means explicit file path (or folder if ends with '/')\n     - \"in [folder]\" means folder\n     - \"as [filename]\" means filename in /tmp unless combined with \"in\"\n   - After removing path-spec tokens, if instructions were NOT set via delimiter or -i/--instructions and there are remaining tokens:\n     - Treat remaining tokens (in original order) as the instructions (fallback mode)\n   - Default output path: `/tmp/promptcode-{preset-name}-{timestamp}.txt` where timestamp is YYYYMMDD-HHMMSS\n\n4. Generate the prompt file:\n   - If instructions are present, determine how to pass them:\n     - For very long instructions (>500 chars): Save to temp file and use --instructions-file\n     - For shorter instructions: Pass directly using --instructions (alias -i)\n   - IMPORTANT: Shell-escape ALL parameters:\n     - Always wrap ALL strings (preset_name, output_path, instructions) in single quotes\n     - Replace any single quote ' inside ANY parameter with '\\'' (close-quote, escaped quote, reopen-quote)\n     - This prevents command injection and ensures security\n   - Command forms:\n\n     ```bash\n     # Without instructions:\n     promptcode generate --preset '{preset_name}' --output '{output_path}'\n     \n     # With short instructions (all values properly escaped with ' replaced by '\\''):\n     promptcode generate --preset '{preset_name}' --output '{output_path}' --instructions '{INSTR_ESC}'\n     \n     # With long instructions (saved to temp file):\n     INSTR_FILE=\"${TMP%/}/instructions-$(date +%Y%m%d-%H%M%S)-$$.txt\"\n     echo '{instructions_content}' > \"$INSTR_FILE\"\n     promptcode generate --preset '{preset_name}' --output '{output_path}' --instructions-file \"$INSTR_FILE\"\n     ```\n\n5. Report results:\n   - Which preset was used (especially important if inferred)\n   - Full path to the output file\n   - Whether instructions were included (show first ~120 chars for confirmation)\n   - Token count and number of files included\n   - Suggest next steps (e.g., \"You can now open this file in your editor\")\n\n## Examples of how users might call this\n\n- `/promptcode-preset-to-prompt functional-framework`\n- `/promptcode-preset-to-prompt functional-framework -- Review for security and performance`\n- `/promptcode-preset-to-prompt functional-framework to ~/Desktop/analysis.txt -- How to migrate to TS 5.6?`\n- `/promptcode-preset-to-prompt functional-framework in ~/Desktop as analysis.txt -- Identify dead code`\n- `/promptcode-preset-to-prompt \"functional framework\" -i \"Focus on memory leaks in parsers\"`\n- `/promptcode-preset-to-prompt microlearning analysis to ~/Desktop/`\n- `/promptcode-preset-to-prompt the functional code as analysis.txt`\n",
  "promptcode-ask-expert.mdc": "---\ndescription: Ask AI expert questions with code context using promptcode\nalwaysApply: false\n---\n\nAsk AI expert questions with code context using promptcode.\n\nWhen asked to consult an expert or analyze code:\n```bash\npromptcode expert \"<question>\" --preset <preset-name>\n```\n\nOr with specific files:\n```bash\npromptcode expert \"<question>\" -f \"src/**/*.ts\"\n```\n\nOptions:\n- `--model <name>`: Choose AI model (default: gpt-5, others: opus-4, sonnet-4, gemini-2.5-pro, grok-4, o3, o3-pro)\n- `--web-search`: Enable web search for current information\n- `--yes`: Skip cost confirmation prompts\n\nNote: The default model is **gpt-5** unless explicitly specified otherwise.\n\nImportant: This requires API keys to be set:\n- `OPENAI_API_KEY` for O3/GPT models\n- `ANTHROPIC_API_KEY` for Claude models\n- `GOOGLE_API_KEY` for Gemini models\n- `XAI_API_KEY` for Grok models\n\nFor expensive operations (>$0.50), always inform the user of the cost before proceeding.",
  "promptcode-preset-create.mdc": "---\ndescription: Create a new promptcode preset from a description\nalwaysApply: false\n---\n\nCreate a new promptcode preset based on a description.\n\n## Basic Usage\n\nWhen asked to create a preset, run:\n```bash\npromptcode preset create <preset-name>\n```\n\nThis creates a basic preset template that you can edit manually.\n\n## Smart Creation with Auto-Optimization\n\nFor better results, create presets from existing files:\n```bash\n# Automatically optimizes patterns from concrete files\npromptcode preset create <preset-name> --from-files \"src/api/**/*.ts\" \"src/utils/**/*.ts\"\n\n# Control optimization level (default: balanced)\npromptcode preset create <preset-name> --from-files \"**/*.ts\" --optimization-level aggressive\n```\n\nOptimization levels:\n- **minimal**: Light optimization, preserves most patterns\n- **balanced**: Default, good balance of pattern reduction\n- **aggressive**: Maximum reduction, fewer patterns\n\n## What It Does\n\n1. Creates preset file in `.promptcode/presets/<preset-name>.patterns`\n2. If using `--from-files`: Analyzes files and generates optimized patterns\n3. Makes preset available for all promptcode commands\n\n## Optimizing Existing Presets\n\nYou can also optimize presets after creation:\n```bash\npromptcode preset optimize <preset-name>           # Preview changes\npromptcode preset optimize <preset-name> --write   # Apply optimization\n```\n\nExample: `promptcode preset create api-endpoints --from-files \"src/api/**/*.ts\"`",
  "promptcode-preset-info.mdc": "---\ndescription: Show details and token count for a specific promptcode preset\nalwaysApply: false\n---\n\nShow details and token count for a specific promptcode preset.\n\nWhen asked about a preset, run:\n```bash\npromptcode preset info <preset-name>\n```\n\nThis displays:\n- File patterns included in the preset\n- Total file count\n- Token count estimate\n- List of matched files\n- Usage examples\n\n## Optimizing Presets\n\nIf the preset has too many patterns or includes too many files, optimize it:\n```bash\npromptcode preset optimize <preset-name>           # Preview changes\npromptcode preset optimize <preset-name> --write   # Apply optimization\n```\n\nOptimization intelligently reduces pattern count while maintaining file coverage.\n\nExample: `promptcode preset info backend`",
  "promptcode-preset-list.mdc": "---\ndescription: List all available promptcode presets with pattern counts\nalwaysApply: false\n---\n\nList all available promptcode presets.\n\nRun the command:\n```bash\npromptcode preset list\n```\n\nThis will display all available presets with their pattern counts. Use the preset names with other promptcode commands to work with specific code contexts.",
  "promptcode-preset-to-prompt.mdc": "---\ndescription: Export a promptcode preset to a file for sharing or archiving\nalwaysApply: false\n---\n\nExport a promptcode preset to a file.\n\nWhen asked to export or save a preset:\n```bash\npromptcode generate --preset <preset-name> --output <output-file>\n```\n\nThis generates a complete prompt with:\n- All file contents from the preset\n- Structured XML-like formatting\n- Token count information\n\nExamples:\n- Export to file: `promptcode generate --preset backend --output backend-context.md`\n- Export with instructions: `promptcode generate --preset api --instructions \"Review for security issues\" --output security-review.md`",
  "promptcode-usage.mdc": "---\ndescription: How to use PromptCode CLI effectively from Cursor\nalwaysApply: false\n---\n\n# PromptCode CLI Usage Guide\n\nPromptCode is a CLI tool that helps generate AI-ready prompts from codebases. When working with PromptCode:\n\n## Core Commands\n\n### Working with Presets\n- `promptcode preset list` - Show all available presets\n- `promptcode preset info <name>` - Get details about a specific preset\n- `promptcode preset create <name>` - Create a new preset\n\n### Generating Prompts\n- `promptcode generate --preset <preset>` - Generate prompt using a preset\n- `promptcode generate -f \"pattern\"` - Generate with file patterns\n- `promptcode generate --output output.md` - Save to file\n\n### Expert Consultation\n- `promptcode expert \"question\"` - Ask AI experts with code context\n- Requires API keys: OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, or XAI_API_KEY\n\n## Pseudo-Commands\n\nYou can use these shortcuts that I'll interpret (matching Claude Code commands):\n- `/promptcode-preset-list` â†’ `promptcode preset list`\n- `/promptcode-preset-info <name>` â†’ `promptcode preset info <name>`\n- `/promptcode-preset-create <name>` â†’ `promptcode preset create <name>`\n- `/promptcode-ask-expert \"question\"` â†’ `promptcode expert \"question\"`\n- `/promptcode-preset-to-prompt <preset>` â†’ `promptcode generate --preset <preset>`\n\n## Important Guidelines\n\n1. **Always request approval** before running commands that modify files\n2. **Check for API keys** when using expert mode\n3. **Parse and summarize** CLI output clearly for the user\n4. **Cost awareness**: For expert consultations over $0.50, always inform the user and get approval\n5. **Use presets** to manage common file pattern groups efficiently\n\n## Common Workflows\n\n### Analyzing a Feature\n1. Create or identify relevant preset: `promptcode preset create feature-name`\n2. Check what it includes: `promptcode preset info feature-name`\n3. Generate context: `promptcode generate --preset feature-name`\n\n### Getting AI Analysis\n1. Ensure API key is set (check environment)\n2. Run expert with context: `promptcode expert \"your question\" --preset <name>`\n3. For expensive models, get user approval first\n\nWhen in doubt, use `promptcode --help` or `promptcode <command> --help` for detailed usage information."
};

export function getEmbeddedTemplates(): Record<string, string> {
  return EMBEDDED_TEMPLATES;
}

export function hasEmbeddedTemplates(): boolean {
  return Object.keys(EMBEDDED_TEMPLATES).length > 0;
}
