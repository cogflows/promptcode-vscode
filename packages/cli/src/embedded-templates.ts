/**
 * Embedded templates for compiled binaries.
 * This file is auto-generated during build - DO NOT EDIT MANUALLY.
 * Generated at: development-build
 */

// Template contents embedded at build time
const EMBEDDED_TEMPLATES: Record<string, string> = {
  "CLAUDE.md.template": "<!-- PROMPTCODE-CLI-START -->\n# PromptCode CLI\n\nAI-ready code analysis via presets and expert consultations.\n\n## Commands\n- `/promptcode-preset-list` - List available presets\n- `/promptcode-preset-info <name>` - Show preset details & tokens\n- `/promptcode-preset-create <description>` - Create preset from description\n- `/promptcode-preset-to-prompt <preset>` - Export preset to file\n- `/promptcode-ask-expert <question>` - AI consultation with code context\n\n## Workflow Examples\n\n### Discovery ‚Üí Context ‚Üí Expert\n```bash\n/promptcode-preset-list                    # Find existing presets\n/promptcode-preset-create auth system      # Or create focused preset\n/promptcode-ask-expert Why is login slow?  # Consult with context\n```\n\n### Direct CLI Usage\n```bash\npromptcode expert \"Review this\" --preset api --yes   # After cost approval\npromptcode generate -f \"src/**/*.ts\" -o prompt.txt   # Export for external use\n```\n\n## Cost Approval Protocol\n1. CLI estimates cost (threshold: $0.50)\n2. CC asks user ONCE for approval\n3. CC re-runs with `--yes` flag\n\n## API Keys Required\n```bash\nexport OPENAI_API_KEY=sk-...     # GPT/O3 models\nexport ANTHROPIC_API_KEY=sk-...  # Claude models\nexport GOOGLE_API_KEY=...        # Gemini models\nexport XAI_API_KEY=...           # Grok models\n```\n\nüí° **Tip**: Create focused presets for better context and lower costs.\n<!-- PROMPTCODE-CLI-END -->",
  "promptcode-ask-expert.md": "---\nallowed-tools: Task, Read(/tmp/*), Write(/tmp/*), Edit(/tmp/*), Edit(.promptcode/presets/*), Bash, Bash(*)\ndescription: Consult AI expert for complex problems with code context - supports ensemble mode for multiple models\n---\n\nConsult an expert about: $ARGUMENTS\n\n## Instructions:\n\n1. Analyze the request in $ARGUMENTS:\n   - Extract the main question/problem\n   - Identify if code context would help (look for keywords about implementation, feature, code review, etc.)\n   - Check for multiple model requests (e.g., \"compare using gpt-5 and opus-4\", \"ask gpt-5, sonnet-4, and gemini\")\n   - Get available models dynamically: `promptcode expert --models --json` (parse the JSON for model list)\n   - If 2+ models detected ‚Üí use ensemble mode\n   - For single model: Use gpt-5 (the default) unless user explicitly specifies another model\n\n2. Determine code context needs:\n   ```bash\n   promptcode preset list\n   ```\n   - Check if an existing preset matches the request (e.g., \"security\" ‚Üí look for security-related presets)\n   - If no suitable preset exists, create one:\n     ```bash\n     promptcode preset create {descriptive-name}\n     ```\n     Then edit `.promptcode/presets/{descriptive-name}.patterns` to add relevant file patterns.\n     Or use `--from-files` with specific patterns:\n     ```bash\n     promptcode preset create {descriptive-name} --from-files \"src/**/*.ts\" \"tests/**/*.test.ts\"\n     ```\n   - Verify the preset:\n     ```bash\n     promptcode preset info {preset-name}\n     ```\n\n3. Prepare consultation file for review:\n   - Set temp directory: `TMP=\"${TMPDIR:-/tmp}\"`\n   - Create unique files: `PROMPT_FILE=\"${TMP%/}/expert-consultation-$(date +%Y%m%d-%H%M%S)-$$.txt\"`\n   - Structure the file with:\n     ```markdown\n     # Expert Consultation\n     \n     ## Question\n     {user's question}\n     \n     ## Context\n     {any relevant context or background}\n     \n     ## Code Context\n     ```\n   - Append the code context using the preset:\n     ```bash\n     CODE_FILE=\"${TMP%/}/code-context-$(date +%Y%m%d-%H%M%S)-$$.txt\"\n     promptcode generate --preset \"{preset_name}\" --output \"$CODE_FILE\"\n     cat \"$CODE_FILE\" >> \"$PROMPT_FILE\"\n     ```\n\n4. Open consultation for user review:\n   ```bash\n   # Try cursor first, then code, then EDITOR, then xdg-open/open\n   if command -v cursor &> /dev/null; then\n     cursor \"$PROMPT_FILE\"\n   elif command -v code &> /dev/null; then\n     code \"$PROMPT_FILE\"\n   elif [ -n \"$EDITOR\" ]; then\n     \"$EDITOR\" \"$PROMPT_FILE\"\n   elif command -v xdg-open &> /dev/null; then\n     xdg-open \"$PROMPT_FILE\"\n   elif command -v open &> /dev/null; then\n     open \"$PROMPT_FILE\"\n   else\n     echo \"üìÑ Consultation file created at: $PROMPT_FILE\"\n     echo \"No editor found. Please open the file manually to review.\"\n   fi\n   ```\n   \n5. Estimate cost and get approval:\n   ```bash\n   promptcode expert --prompt-file \"$PROMPT_FILE\" --model <model> --estimate-cost --json\n   ```\n   - Parse JSON for `cost.total` and `tokens.input`\n   - Exit code: 0 = success, 2 = approval required (cost > threshold)\n   \n   **For single model:**\n   - Say: \"Ready to consult {model} using preset '{preset_name}' ({tokens} tokens, ~${cost}). Reply 'yes' to proceed.\"\n   \n   **For ensemble mode (multiple models):**\n   - Run --estimate-cost for each model in parallel\n   - Say: \"Ready for ensemble consultation with {models} ({total_tokens} tokens). Total: ${total_cost} ({model1}: ${cost1}, {model2}: ${cost2}). Reply 'yes' to proceed.\"\n   \n   **Important: Ask for approval ONLY ONCE - after showing cost estimate**\n\n6. Execute based on mode:\n\n   **Single Model Mode:**\n\n   **For fast models (gpt-5, sonnet-4, opus-4, gemini-2.5-pro, grok-4, etc.):**\n   Run in foreground:\n   ```bash\n   promptcode expert --prompt-file \"$PROMPT_FILE\" --model {model} --yes --json\n   ```\n\n   **For long-running models (gpt-5-pro, o3-pro - can take 1-10 minutes):**\n   Use the Task tool for non-blocking execution (idiomatic Claude Code pattern):\n\n   1. Create result file path with concrete absolute path:\n      ```bash\n      RESULT_FILE=\"${TMP%/}/expert-result-$(date +%Y%m%d-%H%M%S)-$$.json\"\n      ```\n\n   2. Inform the user:\n      ```\n      ‚è≥ Starting background consultation with {model}...\n         This may take several minutes (1-10 min). Launching as autonomous Task...\n         Prompt file: $PROMPT_FILE\n         Results will be saved to: $RESULT_FILE\n         You can continue working - the Task will report back when complete.\n      ```\n\n   3. **Invoke the Task tool directly** with concrete paths and explicit tool usage:\n      ```\n      Task(\"Consult {model}: {short_question_summary}\", \"\"\"\nYou have access to: Bash, Read(/tmp/*), Write(/tmp/*).\n\nYour task: Run a long-running AI consultation (1-10 minutes) and report back with results.\n\n**Step 1: Run consultation with timeout**\nUse Bash tool to run:\ntimeout 15m promptcode expert --prompt-file \"{absolute_path_to_PROMPT_FILE}\" --model {model} --yes --output \"{absolute_path_to_RESULT_FILE}\" --json 2>&1\n\nNotes:\n- Replace {absolute_path_to_PROMPT_FILE} with the actual path (e.g., /tmp/expert-consultation-20251011-094021-48499.txt)\n- Replace {absolute_path_to_RESULT_FILE} with the actual path (e.g., /tmp/expert-result-20251011-094021-48499.json)\n- The --json flag provides structured output\n- 15-minute timeout prevents runaway processes\n- If timeout occurs, report \"TIMEOUT\" and exit\n\n**Step 2: Verify result**\nUse Bash tool:\ntest -s \"{absolute_path_to_RESULT_FILE}\" && echo \"SUCCESS\" || echo \"FAILED\"\n\n**Step 3: Read and parse JSON result**\nUse Read tool to read: {absolute_path_to_RESULT_FILE}\n\nParse the JSON for:\n- cost.total (actual cost)\n- tokens.input and tokens.output\n- responseTime (in seconds)\n- The main response text\n\n**Step 4: Report back**\nReturn a structured report:\n- Status: SUCCESS | FAILED | TIMEOUT\n- Model: {model}\n- Cost: $X.XX\n- Tokens: X input, Y output\n- Response time: X.Xs\n- Summary: Brief summary of key insights (2-3 sentences)\n- Result file: {absolute_path_to_RESULT_FILE}\n\n**On failure:**\n- Capture exit code\n- Report error message\n- Include last 50 lines of stderr if available\n\nReturn all information to the main conversation so the user can see the results.\n\"\"\")\n      ```\n\n      **Important**: Replace all placeholder paths with actual absolute paths before invoking Task.\n\n   4. Return immediately so user can continue working\n   5. Task runs autonomously and persists across Claude Code sessions\n   6. When Task completes and reports back, summarize the findings for the user\n   \n   **Ensemble Mode (Parallel Execution):**\n\n   Create unique result files for each model:\n   ```bash\n   RESULT_FILE_1=\"${TMP%/}/expert-{model1}-$(date +%Y%m%d-%H%M%S)-$$.json\"\n   RESULT_FILE_2=\"${TMP%/}/expert-{model2}-$(date +%Y%m%d-%H%M%S)-$$.json\"\n   # Add more for additional models (max 4)\n   ```\n\n   Inform the user:\n   ```\n   ‚è≥ Starting ensemble consultation with {model1}, {model2}...\n      This will run in parallel. Each model may take 1-10 minutes.\n      Launching autonomous Task to orchestrate...\n      You can continue working - the Task will report back with synthesis and winner.\n   ```\n\n   **Invoke a parent Task that spawns parallel sub-tasks:**\n   ```\n   Task(\"Ensemble: {model1} vs {model2} on {short_question}\", \"\"\"\nYou have access to: Task, Bash, Read(/tmp/*), Write(/tmp/*).\n\nYour task: Run parallel AI consultations with multiple models, then synthesize and declare a winner.\n\n**Step 1: Launch parallel sub-tasks**\n\nSpawn these Tasks in parallel (do not wait between them):\n\nTask(\"Consult {model1}\", '''\nYou have access to: Bash, Read(/tmp/*).\n\nRun: timeout 15m promptcode expert --prompt-file \"{absolute_path_to_PROMPT_FILE}\" --model {model1} --yes --output \"{absolute_path_to_RESULT_FILE_1}\" --json 2>&1\n\nThen:\n1. Verify: test -s \"{absolute_path_to_RESULT_FILE_1}\"\n2. Read the JSON result file\n3. Return: {{status: \"SUCCESS|FAILED\", model: \"{model1}\", cost: $X.XX, tokens: \"X in, Y out\", time: X.Xs, file: \"{absolute_path_to_RESULT_FILE_1}\"}}\n''')\n\nTask(\"Consult {model2}\", '''\nYou have access to: Bash, Read(/tmp/*).\n\nRun: timeout 15m promptcode expert --prompt-file \"{absolute_path_to_PROMPT_FILE}\" --model {model2} --yes --output \"{absolute_path_to_RESULT_FILE_2}\" --json 2>&1\n\nThen:\n1. Verify: test -s \"{absolute_path_to_RESULT_FILE_2}\"\n2. Read the JSON result file\n3. Return: {{status: \"SUCCESS|FAILED\", model: \"{model2}\", cost: $X.XX, tokens: \"X in, Y out\", time: X.Xs, file: \"{absolute_path_to_RESULT_FILE_2}\"}}\n''')\n\n**Step 2: Wait for all sub-tasks to complete**\nBoth Tasks will run in parallel. Wait for both to report back.\n\n**Step 3: Read all result files**\nUse Read tool to read:\n- {absolute_path_to_RESULT_FILE_1}\n- {absolute_path_to_RESULT_FILE_2}\n\nParse each JSON for the actual response text, cost, tokens, and timing.\n\n**Step 4: Create synthesis report**\nAnalyze both responses and create a synthesis file:\n\nUse Write tool to create: {absolute_path_to_SYNTHESIS_FILE}\n\nFormat:\n# Ensemble Expert Consultation Results\n\n## Question\n{original_question}\n\n## Expert Responses\n\n### {Model1} - ${actual_cost}, {response_time}s\n**Key Points:**\n- [Extract 3-4 main points from model1's response]\n\n### {Model2} - ${actual_cost}, {response_time}s\n**Key Points:**\n- [Extract 3-4 main points from model2's response]\n\n## Synthesis\n\n**Consensus Points:**\n- [Points where both models agree]\n\n**Divergent Views:**\n- {Model1}: [Unique perspective]\n- {Model2}: [Unique perspective]\n\n**üèÜ WINNER: {winning_model}**\nReason: [Clear, specific reason why this model provided the better answer - e.g., \"More thorough analysis of edge cases\", \"Better practical recommendations\", \"Deeper technical insights\"]\n\n(Or if genuinely tied: \"TIE - Both provided equally valuable but complementary insights\")\n\n**Performance Summary:**\n- Total Cost: ${sum_of_costs}\n- {Model1}: ${cost1}, {time1}s\n- {Model2}: ${cost2}, {time2}s\n- Best Value: {model with best quality/cost ratio}\n\n**Step 5: Report back**\nReturn to main conversation:\n- Which model won and why\n- Key consensus points\n- Total cost and timing\n- Path to synthesis file: {absolute_path_to_SYNTHESIS_FILE}\n\"\"\")\n   ```\n\n   **Important**:\n   - Replace all placeholder paths with actual absolute paths before invoking Task\n   - The sub-tasks spawn in parallel - do not make them sequential\n   - Limit to maximum 4 models for practical reasons\n   - User has already approved costs via --yes flag\n\n7. Handle the response:\n\n   **Single Model Mode (Fast Models - Foreground):**\n   - Command returns JSON with structured output\n   - Parse JSON for cost, tokens, timing, and response\n   - Summarize key insights to user\n   - If result file was created, optionally open in Cursor/editor\n\n   **Single Model Mode (Long-Running - Background Task):**\n   - Task will report back when complete (happens in background)\n   - When Task returns, it provides structured summary\n   - Share the summary with user\n   - Open result file in Cursor if available\n\n   **Ensemble Mode (Background Task):**\n   - Parent Task orchestrates parallel consultations\n   - When Task completes, it provides:\n     - Synthesis file path\n     - Winner declaration with reasoning\n     - Cost and timing breakdown\n   - Share summary with user\n   - Open synthesis file in Cursor if available\n\n   **Error Handling:**\n   - If any model fails in ensemble mode, Task will continue with successful ones and report which failed\n   - If API key missing:\n     ```\n     To use expert consultation, set your OpenAI API key:\n     export OPENAI_API_KEY=sk-...\n     Get your key from: https://platform.openai.com/api-keys\n     ```\n   - For other errors: Report exact error message from Task or CLI\n   - Timeout errors: Report which model timed out (exceeded 15 minutes)\n\n## Important:\n- **Always use presets** - either existing or create new ones for code context\n- **Single approval flow**: Estimate cost ‚Üí Ask user ONCE ‚Üí Execute with --yes\n- **Show the preset name** to the user so they know what context is being used\n- **Default model is gpt-5** - use this unless user explicitly requests another model\n- Discover default model via `promptcode expert --models --json` (look for `defaultModel: \"gpt-5\"`)\n- For ensemble mode: limit to maximum 4 models\n- NEVER automatically add --yes without user approval\n- **Always use --json flag** for structured output (easier parsing, more reliable than text scraping)\n- **Always use --output flag** for file output (more reliable than stdout redirection)\n- **Timeout protection**: All consultations have 15-minute timeout via `timeout 15m` command\n- **Task-based execution**: For long-running models (gpt-5-pro, o3-pro), use the Task tool to spawn an autonomous sub-agent\n  - Pass concrete absolute file paths to Task (not shell variables like $PROMPT_FILE)\n  - Explicitly state tool usage (Bash, Read, Write) in Task prompt\n  - Task provides true non-blocking execution that persists across sessions\n  - Task will report back when complete\n- **Fast vs slow models**: Fast models (gpt-5, sonnet-4, opus-4, etc) take <30s and run in foreground. Long-running models (gpt-5-pro, o3-pro) take 1-10 minutes and should use Task tool\n- **Ensemble execution**: Parent Task spawns parallel sub-tasks (one per model), waits for all, synthesizes, and declares winner\n- **File paths**: Always use absolute paths in Task prompts. Result files should be .json for structured output\n- Reasoning effort defaults to 'high' (set in CLI) - no need to specify",
  "promptcode-preset-create.md": "---\nallowed-tools: Bash(promptcode preset create:*), Bash(promptcode preset info:*), Glob(**/*), Grep, Write(.promptcode/presets/*.patterns), Read(/tmp/*), Write(/tmp/*), Bash, Bash(*)\ndescription: Create a promptcode preset from description\n---\n\nCreate a promptcode preset for: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the description to understand what code to capture:\n   - Look for keywords like package names, features, components, integrations\n   - Identify if it's Python, TypeScript, or mixed code\n   - Determine the scope (single package, cross-package feature, etc.)\n\n2. Research the codebase structure:\n   - Use Glob to explore relevant directories\n   - Use Grep to find related files if needed\n   - Identify the main code locations and any related tests/docs\n\n3. Generate a descriptive preset name:\n   - Use kebab-case (e.g., \"auth-system\", \"microlearning-utils\")\n   - Keep it concise but descriptive\n\n4. Create the preset (automatically optimized from concrete files):\n   ```bash\n   # When you identify specific files, always use --from-files for smart optimization\n   promptcode preset create \"{preset_name}\" --from-files {file-globs...}\n   # default optimization-level is \"balanced\"\n   # to control: --optimization-level minimal|balanced|aggressive\n   ```\n   This creates `.promptcode/presets/{preset_name}.patterns` with optimized patterns.\n\n5. Edit the preset file to add patterns (if needed):\n   - Start with a header comment explaining what the preset captures\n   - Add inclusion patterns for the main code\n   - Add patterns for related tests and documentation\n   - Include common exclusion patterns:\n     - `!**/__pycache__/**`\n     - `!**/*.pyc`\n     - `!**/node_modules/**`\n     - `!**/dist/**`\n     - `!**/build/**`\n\n6. Test and report results:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n   Report the file count and estimated tokens.\n\n## Common Pattern Examples:\n- Python package: `python/cogflows-py/packages/{package}/src/**/*.py`\n- TypeScript component: `ts/next/{site}/components/{component}/**/*.{ts,tsx}`\n- Cross-package feature: Multiple specific paths\n- Tests: `python/cogflows-py/packages/{package}/tests/**/*.py`\n- Documentation: `**/{feature}/**/*.md`",
  "promptcode-preset-info.md": "---\nallowed-tools: Bash(promptcode preset info:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*), Bash, Bash(*)\ndescription: Show detailed information about a promptcode preset\n---\n\nShow detailed information about promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the arguments to identify the preset:\n   - If exact preset name provided (e.g., \"functional-framework\"), use it directly\n   - If description provided, infer the best matching preset:\n     - Run `promptcode preset list` to see available presets\n     - Read header comments from preset files in `.promptcode/presets/` if needed\n     - Match based on keywords and context\n     - Choose the most relevant preset\n\n2. Run the promptcode info command with the determined preset name:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n\n3. If a preset was inferred from description, explain which preset was chosen and why.\n\nThe output will show:\n- Preset name and path\n- Description from header comments\n- File count and token statistics\n- Pattern details\n- Sample files included\n- Usage instructions",
  "promptcode-preset-list.md": "---\nallowed-tools: Bash(promptcode preset list:*)\ndescription: List all available promptcode presets with pattern counts\n---\n\nList all available promptcode presets.\n\nRun the command:\n```bash\npromptcode preset list\n```\n\nThis will display all available presets with their pattern counts. Use the preset names with other promptcode commands to work with specific code contexts.",
  "promptcode-preset-to-prompt.md": "---\nallowed-tools: Bash(promptcode generate:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*), Read(/tmp/*), Write(/tmp/*), Bash, Bash(*)\ndescription: Generate AI-ready prompt file from a promptcode preset with optional instructions\n---\n\nGenerate prompt file from promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse arguments to understand what the user wants:\n   - Extract preset name or description (may be quoted)\n   - Detect explicit instructions delimiter:\n     - If arguments contain \" -- \" (space, two dashes, space), everything after it is the instructions (preserve verbatim)\n     - Else, look for explicit -i or --instructions value; if provided, use it verbatim as instructions\n   - Keep remaining tokens for output path and/or fallback instructions\n\n2. If inferring from description:\n   - Run `promptcode preset list` to see available presets\n   - Read header comments from `.promptcode/presets/*.patterns` files if needed\n   - Match based on keywords and context\n   - Choose the most relevant preset\n\n3. Determine output path and instructions (fallback):\n   - Resolve output path using existing keywords:\n     - \"to [path]\" means explicit file path (or folder if ends with '/')\n     - \"in [folder]\" means folder\n     - \"as [filename]\" means filename in /tmp unless combined with \"in\"\n   - After removing path-spec tokens, if instructions were NOT set via delimiter or -i/--instructions and there are remaining tokens:\n     - Treat remaining tokens (in original order) as the instructions (fallback mode)\n   - Default output path: `/tmp/promptcode-{preset-name}-{timestamp}.txt` where timestamp is YYYYMMDD-HHMMSS\n\n4. Generate the prompt file:\n   - If instructions are present, determine how to pass them:\n     - For very long instructions (>500 chars): Save to temp file and use --instructions-file\n     - For shorter instructions: Pass directly using --instructions (alias -i)\n   - IMPORTANT: Shell-escape ALL parameters:\n     - Always wrap ALL strings (preset_name, output_path, instructions) in single quotes\n     - Replace any single quote ' inside ANY parameter with '\\'' (close-quote, escaped quote, reopen-quote)\n     - This prevents command injection and ensures security\n   - Command forms:\n\n     ```bash\n     # Without instructions:\n     promptcode generate --preset '{preset_name}' --output '{output_path}'\n     \n     # With short instructions (all values properly escaped with ' replaced by '\\''):\n     promptcode generate --preset '{preset_name}' --output '{output_path}' --instructions '{INSTR_ESC}'\n     \n     # With long instructions (saved to temp file):\n     INSTR_FILE=\"${TMP%/}/instructions-$(date +%Y%m%d-%H%M%S)-$$.txt\"\n     echo '{instructions_content}' > \"$INSTR_FILE\"\n     promptcode generate --preset '{preset_name}' --output '{output_path}' --instructions-file \"$INSTR_FILE\"\n     ```\n\n5. Report results:\n   - Which preset was used (especially important if inferred)\n   - Full path to the output file\n   - Whether instructions were included (show first ~120 chars for confirmation)\n   - Token count and number of files included\n   - Suggest next steps (e.g., \"You can now open this file in your editor\")\n\n## Examples of how users might call this\n\n- `/promptcode-preset-to-prompt functional-framework`\n- `/promptcode-preset-to-prompt functional-framework -- Review for security and performance`\n- `/promptcode-preset-to-prompt functional-framework to ~/Desktop/analysis.txt -- How to migrate to TS 5.6?`\n- `/promptcode-preset-to-prompt functional-framework in ~/Desktop as analysis.txt -- Identify dead code`\n- `/promptcode-preset-to-prompt \"functional framework\" -i \"Focus on memory leaks in parsers\"`\n- `/promptcode-preset-to-prompt microlearning analysis to ~/Desktop/`\n- `/promptcode-preset-to-prompt the functional code as analysis.txt`\n",
  "promptcode-ask-expert.mdc": "---\ndescription: Ask AI expert questions with code context using promptcode\nalwaysApply: false\n---\n\nAsk AI expert questions with code context using promptcode.\n\nWhen asked to consult an expert or analyze code:\n```bash\npromptcode expert \"<question>\" --preset <preset-name>\n```\n\nOr with specific files:\n```bash\npromptcode expert \"<question>\" -f \"src/**/*.ts\"\n```\n\nOptions:\n- `--model <name>`: Choose AI model (default: gpt-5, others: opus-4, sonnet-4, gemini-2.5-pro, grok-4, o3, o3-pro, gpt-5-pro)\n- `--web-search`: Enable web search for current information\n- `--yes`: Skip cost confirmation prompts\n- `--output <file>`: Save response to file (required for background execution)\n- `--json`: Return structured JSON output (recommended for parsing)\n\nNote: The default model is **gpt-5** unless explicitly specified otherwise.\n\n## Long-Running Models (gpt-5-pro, o3-pro)\n\nThese models can take 1-10 minutes due to extended thinking time. Use background execution with proper error handling and timeouts:\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Create temporary files with unique names (mktemp is more reliable than /tmp/manual-names)\nRESULT_FILE=\"$(mktemp -t expert-result-XXXXXX.json)\"\nLOG_FILE=\"$(mktemp -t expert-log-XXXXXX.txt)\"\nSTATUS_FILE=\"$(mktemp -t expert-status-XXXXXX.json)\"\n\n# Prepare the command with timeout protection (15 minutes max)\nQUESTION=\"<your question here>\"\nMODEL=\"gpt-5-pro\"  # or o3-pro\n\n# Save initial status\ncat > \"$STATUS_FILE\" << EOF\n{\n  \"status\": \"starting\",\n  \"model\": \"$MODEL\",\n  \"started\": \"$(date +%s)\",\n  \"result\": \"$RESULT_FILE\",\n  \"log\": \"$LOG_FILE\"\n}\nEOF\n\necho \"‚è≥ Starting background consultation with $MODEL...\"\necho \"   Results: $RESULT_FILE\"\necho \"   Log: $LOG_FILE\"\necho \"   Status: $STATUS_FILE\"\n\n# Run in background with nohup, timeout, and proper quoting\n# Note: Use bash -lc to ensure login shell environment (API keys loaded)\nnohup bash -lc \"\n  set -euo pipefail\n  timeout 15m promptcode expert \\\"$QUESTION\\\" \\\n    --model \\\"$MODEL\\\" \\\n    --yes \\\n    --output \\\"$RESULT_FILE\\\" \\\n    --json \\\n    2>&1 | tee -a \\\"$LOG_FILE\\\"\n  EXIT_CODE=\\${PIPESTATUS[0]}\n  if [ \\$EXIT_CODE -eq 0 ]; then\n    echo '{\\\"status\\\":\\\"success\\\",\\\"completed\\\":\\\"'\"\\$(date +%s)\"'\\\"}' >> \\\"$STATUS_FILE\\\"\n  elif [ \\$EXIT_CODE -eq 124 ]; then\n    echo '{\\\"status\\\":\\\"timeout\\\",\\\"completed\\\":\\\"'\"\\$(date +%s)\"'\\\"}' >> \\\"$STATUS_FILE\\\"\n  else\n    echo '{\\\"status\\\":\\\"failed\\\",\\\"exitCode\\\":'\\\"\\$EXIT_CODE\\\"',\\\"completed\\\":\\\"'\"\\$(date +%s)\"'\\\"}' >> \\\"$STATUS_FILE\\\"\n  fi\n\" &\n\nBG_PID=$!\ndisown \"$BG_PID\" 2>/dev/null || true\n\n# Update status with PID\ncat > \"$STATUS_FILE\" << EOF\n{\n  \"status\": \"running\",\n  \"pid\": $BG_PID,\n  \"model\": \"$MODEL\",\n  \"started\": \"$(date +%s)\",\n  \"result\": \"$RESULT_FILE\",\n  \"log\": \"$LOG_FILE\"\n}\nEOF\n\necho \"\"\necho \"‚úì Background consultation launched (PID: $BG_PID)\"\necho \"\"\necho \"Monitor progress:\"\necho \"  tail -f \\\"$LOG_FILE\\\"\"\necho \"\"\necho \"Check status:\"\necho \"  kill -0 $BG_PID 2>/dev/null && echo 'Still running' || echo 'Complete or failed'\"\necho \"  cat \\\"$STATUS_FILE\\\" | jq .\"\necho \"\"\necho \"View results when complete:\"\necho \"  cat \\\"$RESULT_FILE\\\" | jq .\"\necho \"\"\necho \"Note: 15-minute timeout will auto-kill if needed\"\n```\n\n**Monitoring a running consultation:**\n\n```bash\n# Check if still running (replace PID)\nkill -0 <PID> 2>/dev/null && echo \"Running\" || echo \"Stopped\"\n\n# View live progress\ntail -f /tmp/expert-log-XXXXXX.txt\n\n# Check final status\ncat /tmp/expert-status-XXXXXX.json | jq .\n\n# Read result (when complete)\ncat /tmp/expert-result-XXXXXX.json | jq .\n```\n\n## API Keys\n\nThis requires API keys to be set:\n- `OPENAI_API_KEY` for O3/GPT models\n- `ANTHROPIC_API_KEY` for Claude models\n- `GOOGLE_API_KEY` for Gemini models\n- `XAI_API_KEY` for Grok models\n\nFor expensive operations (>$0.50), always inform the user of the cost before proceeding.",
  "promptcode-preset-create.mdc": "---\ndescription: Create a new promptcode preset from a description\nalwaysApply: false\n---\n\nCreate a new promptcode preset based on a description.\n\n## Basic Usage\n\nWhen asked to create a preset, run:\n```bash\npromptcode preset create <preset-name>\n```\n\nThis creates a basic preset template that you can edit manually.\n\n## Smart Creation with Auto-Optimization\n\nFor better results, create presets from existing files:\n```bash\n# Automatically optimizes patterns from concrete files\npromptcode preset create <preset-name> --from-files \"src/api/**/*.ts\" \"src/utils/**/*.ts\"\n\n# Control optimization level (default: balanced)\npromptcode preset create <preset-name> --from-files \"**/*.ts\" --optimization-level aggressive\n```\n\nOptimization levels:\n- **minimal**: Light optimization, preserves most patterns\n- **balanced**: Default, good balance of pattern reduction\n- **aggressive**: Maximum reduction, fewer patterns\n\n## What It Does\n\n1. Creates preset file in `.promptcode/presets/<preset-name>.patterns`\n2. If using `--from-files`: Analyzes files and generates optimized patterns\n3. Makes preset available for all promptcode commands\n\n## Optimizing Existing Presets\n\nYou can also optimize presets after creation:\n```bash\npromptcode preset optimize <preset-name>           # Preview changes\npromptcode preset optimize <preset-name> --write   # Apply optimization\n```\n\nExample: `promptcode preset create api-endpoints --from-files \"src/api/**/*.ts\"`",
  "promptcode-preset-info.mdc": "---\ndescription: Show details and token count for a specific promptcode preset\nalwaysApply: false\n---\n\nShow details and token count for a specific promptcode preset.\n\nWhen asked about a preset, run:\n```bash\npromptcode preset info <preset-name>\n```\n\nThis displays:\n- File patterns included in the preset\n- Total file count\n- Token count estimate\n- List of matched files\n- Usage examples\n\n## Optimizing Presets\n\nIf the preset has too many patterns or includes too many files, optimize it:\n```bash\npromptcode preset optimize <preset-name>           # Preview changes\npromptcode preset optimize <preset-name> --write   # Apply optimization\n```\n\nOptimization intelligently reduces pattern count while maintaining file coverage.\n\nExample: `promptcode preset info backend`",
  "promptcode-preset-list.mdc": "---\ndescription: List all available promptcode presets with pattern counts\nalwaysApply: false\n---\n\nList all available promptcode presets.\n\nRun the command:\n```bash\npromptcode preset list\n```\n\nThis will display all available presets with their pattern counts. Use the preset names with other promptcode commands to work with specific code contexts.",
  "promptcode-preset-to-prompt.mdc": "---\ndescription: Export a promptcode preset to a file for sharing or archiving\nalwaysApply: false\n---\n\nExport a promptcode preset to a file.\n\nWhen asked to export or save a preset:\n```bash\npromptcode generate --preset <preset-name> --output <output-file>\n```\n\nThis generates a complete prompt with:\n- All file contents from the preset\n- Structured XML-like formatting\n- Token count information\n\nExamples:\n- Export to file: `promptcode generate --preset backend --output backend-context.md`\n- Export with instructions: `promptcode generate --preset api --instructions \"Review for security issues\" --output security-review.md`",
  "promptcode-usage.mdc": "---\ndescription: How to use PromptCode CLI effectively from Cursor\nalwaysApply: false\n---\n\n# PromptCode CLI Usage Guide\n\nPromptCode is a CLI tool that helps generate AI-ready prompts from codebases. When working with PromptCode:\n\n## Core Commands\n\n### Working with Presets\n- `promptcode preset list` - Show all available presets\n- `promptcode preset info <name>` - Get details about a specific preset\n- `promptcode preset create <name>` - Create a new preset\n\n### Generating Prompts\n- `promptcode generate --preset <preset>` - Generate prompt using a preset\n- `promptcode generate -f \"pattern\"` - Generate with file patterns\n- `promptcode generate --output output.md` - Save to file\n\n### Expert Consultation\n- `promptcode expert \"question\"` - Ask AI experts with code context\n- Requires API keys: OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, or XAI_API_KEY\n\n## Pseudo-Commands\n\nYou can use these shortcuts that I'll interpret (matching Claude Code commands):\n- `/promptcode-preset-list` ‚Üí `promptcode preset list`\n- `/promptcode-preset-info <name>` ‚Üí `promptcode preset info <name>`\n- `/promptcode-preset-create <name>` ‚Üí `promptcode preset create <name>`\n- `/promptcode-ask-expert \"question\"` ‚Üí `promptcode expert \"question\"`\n- `/promptcode-preset-to-prompt <preset>` ‚Üí `promptcode generate --preset <preset>`\n\n## Important Guidelines\n\n1. **Always request approval** before running commands that modify files\n2. **Check for API keys** when using expert mode\n3. **Parse and summarize** CLI output clearly for the user\n4. **Cost awareness**: For expert consultations over $0.50, always inform the user and get approval\n5. **Use presets** to manage common file pattern groups efficiently\n\n## Common Workflows\n\n### Analyzing a Feature\n1. Create or identify relevant preset: `promptcode preset create feature-name`\n2. Check what it includes: `promptcode preset info feature-name`\n3. Generate context: `promptcode generate --preset feature-name`\n\n### Getting AI Analysis\n1. Ensure API key is set (check environment)\n2. Run expert with context: `promptcode expert \"your question\" --preset <name>`\n3. For expensive models, get user approval first\n\nWhen in doubt, use `promptcode --help` or `promptcode <command> --help` for detailed usage information."
};

export function getEmbeddedTemplates(): Record<string, string> {
  return EMBEDDED_TEMPLATES;
}

export function hasEmbeddedTemplates(): boolean {
  return Object.keys(EMBEDDED_TEMPLATES).length > 0;
}
