/**
 * Embedded templates for compiled binaries.
 * This file is auto-generated during build - DO NOT EDIT MANUALLY.
 * Generated at: development-build
 */

// Template contents embedded at build time
const EMBEDDED_TEMPLATES: Record<string, string> = {
  "CLAUDE.md.template": "<!-- PROMPTCODE-CLI-START -->\n# PromptCode CLI Integration\n\nThis project has PromptCode CLI integrated for AI-assisted code analysis. The CLI provides structured access to the codebase through presets and intelligent commands.\n\n## Available Claude Commands\n\nThe following commands are available to help you work with this codebase:\n\n- `/promptcode-preset-list` - List all available code presets\n- `/promptcode-preset-info <name>` - Show details and token count for a preset\n- `/promptcode-preset-create <description>` - Create a new preset from description\n- `/promptcode-preset-to-prompt <preset>` - Export preset to a file\n- `/promptcode-ask-expert <question>` - Consult AI expert with code context\n\n## Quick Examples\n\n```bash\n# See what presets are available\n/promptcode-preset-list\n\n# Get details about a specific preset\n/promptcode-preset-info auth-system\n\n# Create a preset for a feature\n/promptcode-preset-create authentication and authorization system\n\n# Ask an expert about the code\n/promptcode-ask-expert How does the authentication flow work?\n```\n\n## Direct CLI Usage\n\nFor simple operations, you can also use the CLI directly:\n\n```bash\n# Generate a prompt from files\npromptcode generate -f \"src/**/*.ts\" -o analysis.txt\n\n# Quick expert consultation (requires API key)\npromptcode expert \"Find security issues\" --preset api --yes\n\n# View preset information with JSON output\npromptcode preset info backend --json\n```\n\n## Configuration\n\nSet API keys via environment variables for expert consultations:\n```bash\nexport OPENAI_API_KEY=sk-...      # For O3/O3-pro models\nexport ANTHROPIC_API_KEY=sk-...   # For Claude models\nexport GOOGLE_API_KEY=...         # For Gemini models\nexport XAI_API_KEY=...            # For Grok models\n```\n\n## Cost Protection\n\nThe expert command has built-in cost protection:\n- Operations over $0.50 require explicit approval\n- Premium models (e.g., o3-pro) always require confirmation\n- Use `--yes` flag only after getting user approval\n\n<details>\n<summary>‚ö†Ô∏è Troubleshooting</summary>\n\n‚Ä¢ **Command not found** ‚Äì The CLI auto-installs to `~/.local/bin`. Ensure it's in PATH  \n‚Ä¢ **Missing API key** ‚Äì Set environment variables as shown above  \n‚Ä¢ **Context too large** ‚Äì Use more specific file patterns or focused presets  \n‚Ä¢ **Preset not found** ‚Äì Check `.promptcode/presets/` directory exists\n</details>\n<!-- PROMPTCODE-CLI-END -->",
  "promptcode-ask-expert.md": "---\nallowed-tools: Bash(promptcode expert:*), Bash(promptcode preset:*), Bash(promptcode generate:*), Bash(open:*), Read(/tmp/*), Write(/tmp/*), Task, Bash(command -v:*), Bash(cursor:*), Bash(code:*), Bash(echo:*), Bash(cat:*), Bash(wait:*), Bash([ -n:*), Bash(test:*)\ndescription: Consult AI expert for complex problems with code context - supports ensemble mode for multiple models\n---\n\nConsult an expert about: $ARGUMENTS\n\n## Instructions:\n\n1. Analyze the request in $ARGUMENTS:\n   - Extract the main question/problem\n   - Identify if code context would help (look for keywords about implementation, feature, code review, etc.)\n   - Check for multiple model requests (e.g., \"compare using o3 and gpt-5\", \"ask o3, gpt-5, and gemini\")\n   - Get available models dynamically: `promptcode expert --models --json` (parse the JSON for model list)\n   - If 2+ models detected ‚Üí use ensemble mode\n   - For single model: determine preference (if user mentions \"o3-pro\" or \"o3 pro\", use o3-pro)\n\n2. Determine code context needs:\n   ```bash\n   promptcode preset list\n   ```\n   - Check if an existing preset matches the request (e.g., \"security\" ‚Üí look for security-related presets)\n   - If no suitable preset exists, use the `/promptcode-preset-create` command:\n     ```\n     /promptcode-preset-create {description of what code to include based on the question}\n     ```\n     This will intelligently create a preset with the right patterns.\n   - Verify the preset:\n     ```bash\n     promptcode preset info {preset-name}\n     ```\n\n3. Prepare consultation file for review:\n   - Create a consultation file at `/tmp/expert-consultation-{timestamp}.md`\n   - Structure the file with:\n     ```markdown\n     # Expert Consultation\n     \n     ## Question\n     {user's question}\n     \n     ## Context\n     {any relevant context or background}\n     \n     ## Code Context\n     ```\n   - Append the code context using the preset:\n     ```bash\n     promptcode generate --preset \"{preset_name}\" -o /tmp/code-context-{timestamp}.txt\n     cat /tmp/code-context-{timestamp}.txt >> \"/tmp/expert-consultation-{timestamp}.md\"\n     ```\n\n4. Open consultation for user review:\n   ```bash\n   # Try cursor first, then code, then EDITOR, then fallback to cat\n   if command -v cursor &> /dev/null; then\n     cursor \"/tmp/expert-consultation-{timestamp}.md\"\n   elif command -v code &> /dev/null; then\n     code \"/tmp/expert-consultation-{timestamp}.md\"\n   elif [ -n \"$EDITOR\" ]; then\n     \"$EDITOR\" \"/tmp/expert-consultation-{timestamp}.md\"\n   else\n     echo \"üìÑ Consultation file created at: /tmp/expert-consultation-{timestamp}.md\"\n     echo \"No editor found. Please open the file manually to review.\"\n   fi\n   ```\n   \n5. Estimate cost and get approval:\n   - Use the CLI's built-in cost estimation:\n     ```bash\n     promptcode expert --prompt-file \"/tmp/expert-consultation-{timestamp}.md\" --model <model> --estimate-cost --json\n     ```\n   - Parse the JSON output to get:\n     - `tokens.input` - total input tokens\n     - `cost.total` - estimated total cost\n   - Check the exit code: 0 = success, 2 = approval required (cost > threshold)\n   \n   **For single model:**\n   - Say: \"I've prepared the expert consultation using preset '{preset_name}' (~{tokens} tokens). Model: {model}. The consultation file is open in Cursor for review. Reply 'yes' to send to the expert (estimated cost: ${cost from CLI}).\"\n   \n   **For ensemble mode (multiple models):**\n   - Run --estimate-cost for each model in parallel to get costs\n   - Say: \"I've prepared an ensemble consultation using preset '{preset_name}' (~{tokens} tokens) with {models}. Total estimated cost: ${total_cost} ({model1}: ${cost1}, {model2}: ${cost2}, ...). The consultation file is open for review. Reply 'yes' to proceed with all models in parallel.\"\n\n6. Execute based on mode:\n\n   **Single Model Mode:**\n   ```bash\n   promptcode expert --prompt-file \"/tmp/expert-consultation-{timestamp}.md\" --model {model} --yes\n   ```\n   \n   **Ensemble Mode (Parallel Execution):**\n   - Use a SINGLE parent Task that orchestrates parallel sub-tasks (idiomatic for Claude Code)\n   - The parent Task:\n     1. Launches parallel sub-tasks for each model\n     2. Waits for all sub-tasks to complete\n     3. Reads all response files\n     4. Creates the synthesis report (Step 7)\n   - Structure:\n     ```\n     Task: \"Ensemble consultation with {model1} and {model2}\"\n     Prompt: \"\n       Step 1: Run these consultations in PARALLEL as sub-tasks:\n       - Sub-task 1: promptcode expert --prompt-file '/tmp/expert-consultation-{timestamp}.md' --model {model1} --yes > /tmp/expert-{model1}-{timestamp}.txt 2>&1\n       - Sub-task 2: promptcode expert --prompt-file '/tmp/expert-consultation-{timestamp}.md' --model {model2} --yes > /tmp/expert-{model2}-{timestamp}.txt 2>&1\n       \n       Step 2: After both complete, read the response files\n       Step 3: Create synthesis report as described in Step 7\n       Step 4: Report back with synthesis and winner\n     \"\n     ```\n   - Note: The --yes flag confirms we have user approval for the cost\n   - The allowed-tools configuration permits these commands to run without additional prompts\n\n7. Handle the response:\n\n   **Single Model Mode:**\n   - If successful: Open response in Cursor (if available) and summarize key insights\n   - If API key missing: Show appropriate setup instructions\n   \n   **Ensemble Mode (Synthesis):**\n   - Read all response text files\n   - Extract key insights from each model's response\n   - Create synthesis report in `/tmp/expert-ensemble-synthesis-{timestamp}.md`:\n   \n   ```markdown\n   # Ensemble Expert Consultation Results\n   \n   ## Question\n   {original_question}\n   \n   ## Expert Responses\n   \n   ### {Model1} - ${actual_cost}, {response_time}s\n   **Key Points:**\n   - {key_point_1}\n   - {key_point_2}\n   - {key_point_3}\n   \n   ### {Model2} - ${actual_cost}, {response_time}s\n   **Key Points:**\n   - {key_point_1}\n   - {key_point_2}\n   - {key_point_3}\n   \n   ## Synthesis\n   \n   **Consensus Points:**\n   - {point_agreed_by_multiple_models}\n   - {another_consensus_point}\n   \n   **Best Comprehensive Answer:** {Model} provided the most thorough analysis, particularly strong on {specific_aspect}\n   \n   **Unique Insights:**\n   - {Model1}: {unique_insight_from_model1}\n   - {Model2}: {unique_insight_from_model2}\n   \n   **üèÜ WINNER:** {winning_model} - {clear_reason_why_this_model_won}\n   (If tie: \"TIE - Both models provided equally valuable but complementary insights\")\n   \n   **Performance Summary:**\n   - Total Cost: ${total_actual_cost}\n   - Total Time: {total_time}s\n   - Best Value: {model_with_best_cost_to_quality_ratio}\n   ```\n   \n   - Open synthesis in Cursor if available\n   - IMPORTANT: Always declare a clear winner (or explicitly state if it's a tie)\n   - Provide brief summary of which model performed best and why they won\n\n   **Error Handling:**\n   - If any model fails in ensemble mode, continue with successful ones\n   - Report which models succeeded/failed\n   - If OPENAI_API_KEY missing:\n     ```\n     To use expert consultation, set your OpenAI API key:\n     export OPENAI_API_KEY=sk-...\n     Get your key from: https://platform.openai.com/api-keys\n     ```\n   - For other errors: Report exact error message\n\n## Important:\n- **Always use presets** - either existing or create new ones for code context\n- **Create presets intelligently** - analyze the question to determine which files are relevant\n- **Show the preset name** to the user so they know what context is being used\n- Default to GPT-5 model unless another model is explicitly requested\n- For ensemble mode: limit to maximum 4 models to prevent resource exhaustion\n- Always show cost estimate before sending\n- Keep questions clear and specific\n- Include relevant code context when asking about specific functionality\n- NEVER automatically add --yes/--force without user approval\n- Only ask for approval ONCE before sending to expert (not for preparatory steps)\n- Reasoning effort defaults to 'high' (set in CLI) - no need to specify\n- Use `promptcode generate -o` to avoid stdout redirection issues",
  "promptcode-preset-create.md": "---\nallowed-tools: Bash(promptcode preset create:*), Bash(promptcode preset info:*), Glob(**/*), Grep, Write(.promptcode/presets/*.patterns)\ndescription: Create a promptcode preset from description\n---\n\nCreate a promptcode preset for: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the description to understand what code to capture:\n   - Look for keywords like package names, features, components, integrations\n   - Identify if it's Python, TypeScript, or mixed code\n   - Determine the scope (single package, cross-package feature, etc.)\n\n2. Research the codebase structure:\n   - Use Glob to explore relevant directories\n   - Use Grep to find related files if needed\n   - Identify the main code locations and any related tests/docs\n\n3. Generate a descriptive preset name:\n   - Use kebab-case (e.g., \"auth-system\", \"microlearning-utils\")\n   - Keep it concise but descriptive\n\n4. Create the preset (automatically optimized from concrete files):\n   ```bash\n   # When you identify specific files, always use --from-files for smart optimization\n   promptcode preset create \"{preset_name}\" --from-files {file-globs...}\n   # default optimization-level is \"balanced\"\n   # to control: --optimization-level minimal|balanced|aggressive\n   ```\n   This creates `.promptcode/presets/{preset_name}.patterns` with optimized patterns.\n\n5. Edit the preset file to add patterns (if needed):\n   - Start with a header comment explaining what the preset captures\n   - Add inclusion patterns for the main code\n   - Add patterns for related tests and documentation\n   - Include common exclusion patterns:\n     - `!**/__pycache__/**`\n     - `!**/*.pyc`\n     - `!**/node_modules/**`\n     - `!**/dist/**`\n     - `!**/build/**`\n\n6. Test and report results:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n   Report the file count and estimated tokens.\n\n## Common Pattern Examples:\n- Python package: `python/cogflows-py/packages/{package}/src/**/*.py`\n- TypeScript component: `ts/next/{site}/components/{component}/**/*.{ts,tsx}`\n- Cross-package feature: Multiple specific paths\n- Tests: `python/cogflows-py/packages/{package}/tests/**/*.py`\n- Documentation: `**/{feature}/**/*.md`",
  "promptcode-preset-info.md": "---\nallowed-tools: Bash(promptcode preset info:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*)\ndescription: Show detailed information about a promptcode preset\n---\n\nShow detailed information about promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the arguments to identify the preset:\n   - If exact preset name provided (e.g., \"functional-framework\"), use it directly\n   - If description provided, infer the best matching preset:\n     - Run `promptcode preset list` to see available presets\n     - Read header comments from preset files in `.promptcode/presets/` if needed\n     - Match based on keywords and context\n     - Choose the most relevant preset\n\n2. Run the promptcode info command with the determined preset name:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n\n3. If a preset was inferred from description, explain which preset was chosen and why.\n\nThe output will show:\n- Preset name and path\n- Description from header comments\n- File count and token statistics\n- Pattern details\n- Sample files included\n- Usage instructions",
  "promptcode-preset-list.md": "---\nallowed-tools: Bash(promptcode preset list:*)\ndescription: List all available promptcode presets with pattern counts\n---\n\nList all available promptcode presets.\n\nRun the command:\n```bash\npromptcode preset list\n```\n\nThis will display all available presets with their pattern counts. Use the preset names with other promptcode commands to work with specific code contexts.",
  "promptcode-preset-to-prompt.md": "---\nallowed-tools: Bash(promptcode generate:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*)\ndescription: Generate AI-ready prompt file from a promptcode preset\n---\n\nGenerate prompt file from promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse arguments to understand what the user wants:\n   - Extract preset name or description\n   - Extract output path/filename if specified (e.g., \"to ~/Desktop/analysis.txt\", \"in /tmp/\", \"as myfile.txt\")\n\n2. If inferring from description:\n   - Run `promptcode preset list` to see available presets\n   - Read header comments from `.promptcode/presets/*.patterns` files if needed\n   - Match based on keywords and context\n   - Choose the most relevant preset\n\n3. Determine output path:\n   - Default: `/tmp/promptcode-{preset-name}-{timestamp}.txt` where timestamp is YYYYMMDD-HHMMSS\n   - If user specified just a folder: `{folder}/promptcode-{preset-name}-{timestamp}.txt`\n   - If user specified filename without path: `/tmp/{filename}`\n   - If user specified full path: use exactly as specified\n\n4. Generate the prompt file:\n   ```bash\n   promptcode generate --preset \"{preset_name}\" --output \"{output_path}\"\n   ```\n\n5. Report results:\n   - Which preset was used (especially important if inferred)\n   - Full path to the output file\n   - Token count and number of files included\n   - Suggest next steps (e.g., \"You can now open this file in your editor\")\n\n## Examples of how users might call this:\n- `/promptcode-preset-to-prompt functional-framework`\n- `/promptcode-preset-to-prompt microlearning analysis to ~/Desktop/`\n- `/promptcode-preset-to-prompt the functional code as analysis.txt`",
  "promptcode-ask-expert.mdc": "---\ndescription: Ask AI expert questions with code context using promptcode\nalwaysApply: false\n---\n\nAsk AI expert questions with code context using promptcode.\n\nWhen asked to consult an expert or analyze code:\n```bash\npromptcode expert \"<question>\" --preset <preset-name>\n```\n\nOr with specific files:\n```bash\npromptcode expert \"<question>\" -f \"src/**/*.ts\"\n```\n\nOptions:\n- `--model <name>`: Choose AI model (gpt-5, o3, opus-4, sonnet-4, gemini-2.5-pro, grok-4)\n- `--web-search`: Enable web search for current information\n- `--yes`: Skip cost confirmation prompts\n\nImportant: This requires API keys to be set:\n- `OPENAI_API_KEY` for O3/GPT models\n- `ANTHROPIC_API_KEY` for Claude models\n- `GOOGLE_API_KEY` for Gemini models\n- `XAI_API_KEY` for Grok models\n\nFor expensive operations (>$0.50), always inform the user of the cost before proceeding.",
  "promptcode-preset-create.mdc": "---\ndescription: Create a new promptcode preset from a description\nalwaysApply: false\n---\n\nCreate a new promptcode preset based on a description.\n\n## Basic Usage\n\nWhen asked to create a preset, run:\n```bash\npromptcode preset create <preset-name>\n```\n\nThis creates a basic preset template that you can edit manually.\n\n## Smart Creation with Auto-Optimization\n\nFor better results, create presets from existing files:\n```bash\n# Automatically optimizes patterns from concrete files\npromptcode preset create <preset-name> --from-files \"src/api/**/*.ts\" \"src/utils/**/*.ts\"\n\n# Control optimization level (default: balanced)\npromptcode preset create <preset-name> --from-files \"**/*.ts\" --optimization-level aggressive\n```\n\nOptimization levels:\n- **minimal**: Light optimization, preserves most patterns\n- **balanced**: Default, good balance of pattern reduction\n- **aggressive**: Maximum reduction, fewer patterns\n\n## What It Does\n\n1. Creates preset file in `.promptcode/presets/<preset-name>.patterns`\n2. If using `--from-files`: Analyzes files and generates optimized patterns\n3. Makes preset available for all promptcode commands\n\n## Optimizing Existing Presets\n\nYou can also optimize presets after creation:\n```bash\npromptcode preset optimize <preset-name>           # Preview changes\npromptcode preset optimize <preset-name> --write   # Apply optimization\n```\n\nExample: `promptcode preset create api-endpoints --from-files \"src/api/**/*.ts\"`",
  "promptcode-preset-info.mdc": "---\ndescription: Show details and token count for a specific promptcode preset\nalwaysApply: false\n---\n\nShow details and token count for a specific promptcode preset.\n\nWhen asked about a preset, run:\n```bash\npromptcode preset info <preset-name>\n```\n\nThis displays:\n- File patterns included in the preset\n- Total file count\n- Token count estimate\n- List of matched files\n- Usage examples\n\n## Optimizing Presets\n\nIf the preset has too many patterns or includes too many files, optimize it:\n```bash\npromptcode preset optimize <preset-name>           # Preview changes\npromptcode preset optimize <preset-name> --write   # Apply optimization\n```\n\nOptimization intelligently reduces pattern count while maintaining file coverage.\n\nExample: `promptcode preset info backend`",
  "promptcode-preset-list.mdc": "---\ndescription: List all available promptcode presets with pattern counts\nalwaysApply: false\n---\n\nList all available promptcode presets.\n\nRun the command:\n```bash\npromptcode preset list\n```\n\nThis will display all available presets with their pattern counts. Use the preset names with other promptcode commands to work with specific code contexts.",
  "promptcode-preset-to-prompt.mdc": "---\ndescription: Export a promptcode preset to a file for sharing or archiving\nalwaysApply: false\n---\n\nExport a promptcode preset to a file.\n\nWhen asked to export or save a preset:\n```bash\npromptcode generate -l <preset-name> -o <output-file>\n```\n\nThis generates a complete prompt with:\n- All file contents from the preset\n- Structured XML-like formatting\n- Token count information\n\nExamples:\n- Export to file: `promptcode generate -l backend -o backend-context.md`\n- Export with instructions: `promptcode generate -l api --instructions \"Review for security issues\" -o security-review.md`",
  "promptcode-usage.mdc": "---\ndescription: How to use PromptCode CLI effectively from Cursor\nalwaysApply: false\n---\n\n# PromptCode CLI Usage Guide\n\nPromptCode is a CLI tool that helps generate AI-ready prompts from codebases. When working with PromptCode:\n\n## Core Commands\n\n### Working with Presets\n- `promptcode preset list` - Show all available presets\n- `promptcode preset info <name>` - Get details about a specific preset\n- `promptcode preset create <name>` - Create a new preset\n\n### Generating Prompts\n- `promptcode generate -l <preset>` - Generate prompt using a preset\n- `promptcode generate -f \"pattern\"` - Generate with file patterns\n- `promptcode generate -o output.md` - Save to file\n\n### Expert Consultation\n- `promptcode expert \"question\"` - Ask AI experts with code context\n- Requires API keys: OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, or XAI_API_KEY\n\n## Pseudo-Commands\n\nYou can use these shortcuts that I'll interpret (matching Claude Code commands):\n- `/promptcode-preset-list` ‚Üí `promptcode preset list`\n- `/promptcode-preset-info <name>` ‚Üí `promptcode preset info <name>`\n- `/promptcode-preset-create <name>` ‚Üí `promptcode preset create <name>`\n- `/promptcode-ask-expert \"question\"` ‚Üí `promptcode expert \"question\"`\n- `/promptcode-preset-to-prompt <preset>` ‚Üí `promptcode generate -l <preset>`\n\n## Important Guidelines\n\n1. **Always request approval** before running commands that modify files\n2. **Check for API keys** when using expert mode\n3. **Parse and summarize** CLI output clearly for the user\n4. **Cost awareness**: For expert consultations over $0.50, always inform the user and get approval\n5. **Use presets** to manage common file pattern groups efficiently\n\n## Common Workflows\n\n### Analyzing a Feature\n1. Create or identify relevant preset: `promptcode preset create feature-name`\n2. Check what it includes: `promptcode preset info feature-name`\n3. Generate context: `promptcode generate -l feature-name`\n\n### Getting AI Analysis\n1. Ensure API key is set (check environment)\n2. Run expert with context: `promptcode expert \"your question\" --preset <name>`\n3. For expensive models, get user approval first\n\nWhen in doubt, use `promptcode --help` or `promptcode <command> --help` for detailed usage information."
};

export function getEmbeddedTemplates(): Record<string, string> {
  return EMBEDDED_TEMPLATES;
}

export function hasEmbeddedTemplates(): boolean {
  return Object.keys(EMBEDDED_TEMPLATES).length > 0;
}
