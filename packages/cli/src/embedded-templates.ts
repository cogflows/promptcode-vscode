/**
 * Embedded templates for compiled binaries.
 * This file is auto-generated during build - DO NOT EDIT MANUALLY.
 * Generated at: 2025-08-10T19:22:52.254Z
 */

// Template contents embedded at build time
const EMBEDDED_TEMPLATES: Record<string, string> = {
  "CLAUDE.md.template": "<!-- PROMPTCODE-CLI-START -->\n# PromptCode CLI Integration\n\nThis project has PromptCode CLI integrated for AI-assisted code analysis. The CLI provides structured access to the codebase through presets and intelligent commands.\n\n## Available Claude Commands\n\nThe following commands are available to help you work with this codebase:\n\n- `/promptcode-preset-list` - List all available code presets\n- `/promptcode-preset-info <name>` - Show details and token count for a preset\n- `/promptcode-preset-create <description>` - Create a new preset from description\n- `/promptcode-preset-to-prompt <preset>` - Export preset to a file\n- `/promptcode-ask-expert <question>` - Consult AI expert with code context\n\n## Quick Examples\n\n```bash\n# See what presets are available\n/promptcode-preset-list\n\n# Get details about a specific preset\n/promptcode-preset-info auth-system\n\n# Create a preset for a feature\n/promptcode-preset-create authentication and authorization system\n\n# Ask an expert about the code\n/promptcode-ask-expert How does the authentication flow work?\n```\n\n## Direct CLI Usage\n\nFor simple operations, you can also use the CLI directly:\n\n```bash\n# Generate a prompt from files\npromptcode generate -f \"src/**/*.ts\" -o analysis.txt\n\n# Quick expert consultation (requires API key)\npromptcode expert \"Find security issues\" --preset api --yes\n\n# View preset information with JSON output\npromptcode preset info backend --json\n```\n\n## Configuration\n\nSet API keys via environment variables for expert consultations:\n```bash\nexport OPENAI_API_KEY=sk-...      # For O3/O3-pro models\nexport ANTHROPIC_API_KEY=sk-...   # For Claude models\nexport GOOGLE_API_KEY=...         # For Gemini models\nexport XAI_API_KEY=...            # For Grok models\n```\n\n## Cost Protection\n\nThe expert command has built-in cost protection:\n- Operations over $0.50 require explicit approval\n- Premium models (e.g., o3-pro) always require confirmation\n- Use `--yes` flag only after getting user approval\n\n<details>\n<summary>⚠️ Troubleshooting</summary>\n\n• **Command not found** – The CLI auto-installs to `~/.local/bin`. Ensure it's in PATH  \n• **Missing API key** – Set environment variables as shown above  \n• **Context too large** – Use more specific file patterns or focused presets  \n• **Preset not found** – Check `.promptcode/presets/` directory exists\n</details>\n<!-- PROMPTCODE-CLI-END -->",
  "expert-consultation.md": "---\nallowed-tools: Bash(promptcode expert:*), Bash(promptcode preset:*), Bash(open -a Cursor:*), Read(/tmp/expert-*:*)\ndescription: Consult OpenAI o3/o3-pro expert for complex problems with code context\n---\nConsult an expert about: $ARGUMENTS\n\nInstructions:\n1. Analyze the request in $ARGUMENTS to understand:\n   - The main question/problem\n   - Whether code context would help\n\n2. If code context would be helpful:\n   ```bash\n   promptcode preset list  # See available presets\n   ```\n   \n   Choose relevant preset(s) or create specific file patterns.\n\n3. Ask the expert with appropriate context:\n   ```bash\n   # With preset:\n   promptcode expert \"YOUR_CLEAR_QUESTION\" --preset <preset-name> --model <model>\n   \n   # With specific files:\n   promptcode expert \"YOUR_CLEAR_QUESTION\" -f \"src/**/*.ts\" --model <model>\n   \n   # Without context (general question):\n   promptcode expert \"YOUR_CLEAR_QUESTION\" --model <model>\n   ```\n   \n   The CLI will show estimated cost and ask for confirmation if:\n   - Cost exceeds $0.50\n   - Using a \"pro\" model\n   \n   IMPORTANT: If you see \"Non-interactive environment detected\":\n   - DO NOT automatically add --yes or --no-confirm\n   - STOP and inform the user about the cost\n   - Ask: \"This will cost approximately $X.XX. Do you want to proceed?\"\n   - Only proceed with --yes after user explicitly approves\n   \n   Note: --no-confirm is an auto-accept mode for users who want to bypass\n   all confirmations. AI agents should use --yes after explicit approval.\n   \n   Model options:\n   - `o3` - Standard O3 model ($2/$8 per million tokens)\n   - `o3-pro` - O3 Pro for complex tasks ($20/$80 per million tokens)\n   - If question mentions \"o3-pro\" or \"o3 pro\", use `--model o3-pro`\n   - Otherwise default to `--model o3`\n\n4. If output file was specified, open it:\n   ```bash\n   promptcode expert \"...\" --output response.md\n   open -a Cursor response.md  # or read the file\n   ```\n\n5. Parse the response:\n   - If successful: Summarize key insights\n   - If API key missing: Tell user to set environment variable:\n     ```bash\n     export OPENAI_API_KEY=sk-...\n     # Or for other providers:\n     export ANTHROPIC_API_KEY=sk-ant-...\n     export GOOGLE_API_KEY=...\n     export XAI_API_KEY=...\n     ```\n   - For other errors: Report exact error message\n\nIMPORTANT: \n- Always include relevant code context when asking about specific functionality\n- Be clear and specific in your questions\n- Choose o3-pro only for genuinely complex tasks requiring deep reasoning",
  "promptcode-ask-expert.md": "---\nallowed-tools: Bash(promptcode expert:*), Bash(promptcode preset list:*), Bash(promptcode generate:*), Bash(open -a Cursor:*), Read(/tmp/expert-*:*), Write(/tmp/expert-consultation-*.md)\ndescription: Consult AI expert (O3/O3-pro) for complex problems with code context\n---\n\nConsult an expert about: $ARGUMENTS\n\n## Instructions:\n\n1. Analyze the request in $ARGUMENTS:\n   - Extract the main question/problem\n   - Identify if code context would help (look for keywords matching our presets)\n   - Determine model preference (if user mentions \"o3-pro\" or \"o3 pro\", use o3-pro)\n\n2. If code context needed, list available presets:\n   ```bash\n   promptcode preset list\n   ```\n   Choose relevant preset(s) based on the question.\n\n3. Prepare consultation file for review:\n   - Create a consultation file at `/tmp/expert-consultation-{timestamp}.md`\n   - Structure the file with:\n     ```markdown\n     # Expert Consultation\n     \n     ## Question\n     {user's question}\n     \n     ## Context\n     {any relevant context or background}\n     ```\n   - If a preset would help, append the code context:\n     ```bash\n     echo -e \"\\n## Code Context\\n\" >> \"/tmp/expert-consultation-{timestamp}.md\"\n     promptcode generate --preset \"{preset_name}\" >> \"/tmp/expert-consultation-{timestamp}.md\"\n     ```\n\n4. Open consultation for user review (if Cursor is available):\n   ```bash\n   open -a Cursor \"/tmp/expert-consultation-{timestamp}.md\"\n   ```\n   \n5. Estimate cost and get approval:\n   - Model costs:\n     - O3: $2/$8 per million tokens (input/output)\n     - O3-pro: $20/$80 per million tokens (input/output)\n   - Calculate based on file size (roughly: file_size_bytes / 4 = tokens)\n   - Say: \"I've prepared the expert consultation (~{tokens} tokens). Model: {model}. You can edit the file to refine your question. Reply 'yes' to send to the expert (estimated cost: ${cost}).\"\n\n6. On user approval, send to expert using the prompt file:\n   ```bash\n   promptcode expert --prompt-file \"/tmp/expert-consultation-{timestamp}.md\" --model {model} --yes\n   ```\n   \n   Note: The --yes flag confirms we have user approval for the cost.\n   The --prompt-file approach allows the user to edit the consultation before sending.\n\n7. Handle the response:\n   - If successful: Open response in Cursor (if available) and summarize key insights\n   - If OPENAI_API_KEY missing:\n     ```\n     To use expert consultation, set your OpenAI API key:\n     export OPENAI_API_KEY=sk-...\n     Get your key from: https://platform.openai.com/api-keys\n     ```\n   - For other errors: Report exact error message\n\n## Important:\n- Default to O3 model unless O3-pro explicitly requested or needed for complex reasoning\n- Always show cost estimate before sending\n- Keep questions clear and specific\n- Include relevant code context when asking about specific functionality\n- NEVER automatically add --yes without user approval",
  "promptcode-preset-create.md": "---\nallowed-tools: Bash(promptcode preset create:*), Bash(promptcode preset info:*), Glob(**/*), Grep, Write(.promptcode/presets/*.patterns)\ndescription: Create a promptcode preset from description\n---\n\nCreate a promptcode preset for: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the description to understand what code to capture:\n   - Look for keywords like package names, features, components, integrations\n   - Identify if it's Python, TypeScript, or mixed code\n   - Determine the scope (single package, cross-package feature, etc.)\n\n2. Research the codebase structure:\n   - Use Glob to explore relevant directories\n   - Use Grep to find related files if needed\n   - Identify the main code locations and any related tests/docs\n\n3. Generate a descriptive preset name:\n   - Use kebab-case (e.g., \"auth-system\", \"microlearning-utils\")\n   - Keep it concise but descriptive\n\n4. Create the preset:\n   ```bash\n   promptcode preset create \"{preset_name}\"\n   ```\n   This creates `.promptcode/presets/{preset_name}.patterns`\n\n5. Edit the preset file to add patterns:\n   - Start with a header comment explaining what the preset captures\n   - Add inclusion patterns for the main code\n   - Add patterns for related tests and documentation\n   - Include common exclusion patterns:\n     - `!**/__pycache__/**`\n     - `!**/*.pyc`\n     - `!**/node_modules/**`\n     - `!**/dist/**`\n     - `!**/build/**`\n\n6. Test and report results:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n   Report the file count and estimated tokens.\n\n## Common Pattern Examples:\n- Python package: `python/cogflows-py/packages/{package}/src/**/*.py`\n- TypeScript component: `ts/next/{site}/components/{component}/**/*.{ts,tsx}`\n- Cross-package feature: Multiple specific paths\n- Tests: `python/cogflows-py/packages/{package}/tests/**/*.py`\n- Documentation: `**/{feature}/**/*.md`",
  "promptcode-preset-info.md": "---\nallowed-tools: Bash(promptcode preset info:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*)\ndescription: Show detailed information about a promptcode preset\n---\n\nShow detailed information about promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the arguments to identify the preset:\n   - If exact preset name provided (e.g., \"functional-framework\"), use it directly\n   - If description provided, infer the best matching preset:\n     - Run `promptcode preset list` to see available presets\n     - Read header comments from preset files in `.promptcode/presets/` if needed\n     - Match based on keywords and context\n     - Choose the most relevant preset\n\n2. Run the promptcode info command with the determined preset name:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n\n3. If a preset was inferred from description, explain which preset was chosen and why.\n\nThe output will show:\n- Preset name and path\n- Description from header comments\n- File count and token statistics\n- Pattern details\n- Sample files included\n- Usage instructions",
  "promptcode-preset-list.md": "---\nallowed-tools: Bash(promptcode preset list:*)\ndescription: List all available promptcode presets with pattern counts\n---\n\nList all available promptcode presets.\n\nRun the command:\n```bash\npromptcode preset list\n```\n\nThis will display all available presets with their pattern counts. Use the preset names with other promptcode commands to work with specific code contexts.",
  "promptcode-preset-to-prompt.md": "---\nallowed-tools: Bash(promptcode generate:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*)\ndescription: Generate AI-ready prompt file from a promptcode preset\n---\n\nGenerate prompt file from promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse arguments to understand what the user wants:\n   - Extract preset name or description\n   - Extract output path/filename if specified (e.g., \"to ~/Desktop/analysis.txt\", \"in /tmp/\", \"as myfile.txt\")\n\n2. If inferring from description:\n   - Run `promptcode preset list` to see available presets\n   - Read header comments from `.promptcode/presets/*.patterns` files if needed\n   - Match based on keywords and context\n   - Choose the most relevant preset\n\n3. Determine output path:\n   - Default: `/tmp/promptcode-{preset-name}-{timestamp}.txt` where timestamp is YYYYMMDD-HHMMSS\n   - If user specified just a folder: `{folder}/promptcode-{preset-name}-{timestamp}.txt`\n   - If user specified filename without path: `/tmp/{filename}`\n   - If user specified full path: use exactly as specified\n\n4. Generate the prompt file:\n   ```bash\n   promptcode generate --preset \"{preset_name}\" --output \"{output_path}\"\n   ```\n\n5. Report results:\n   - Which preset was used (especially important if inferred)\n   - Full path to the output file\n   - Token count and number of files included\n   - Suggest next steps (e.g., \"You can now open this file in your editor\")\n\n## Examples of how users might call this:\n- `/promptcode-preset-to-prompt functional-framework`\n- `/promptcode-preset-to-prompt microlearning analysis to ~/Desktop/`\n- `/promptcode-preset-to-prompt the functional code as analysis.txt`"
};

export function getEmbeddedTemplates(): Record<string, string> {
  return EMBEDDED_TEMPLATES;
}

export function hasEmbeddedTemplates(): boolean {
  return Object.keys(EMBEDDED_TEMPLATES).length > 0;
}
