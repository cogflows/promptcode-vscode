/**
 * Embedded templates for compiled binaries.
 * This file is auto-generated during build - DO NOT EDIT MANUALLY.
 * Generated at: development-build
 */

// Template contents embedded at build time
const EMBEDDED_TEMPLATES: Record<string, string> = {
  "CLAUDE.md.template": "<!-- PROMPTCODE-CLI-START -->\n# PromptCode CLI\n\nAI-ready code analysis via presets and expert consultations.\n\n## Commands\n- `/promptcode-preset-list` - List available presets\n- `/promptcode-preset-info <name>` - Show preset details & tokens\n- `/promptcode-preset-create <description>` - Create preset from description\n- `/promptcode-preset-to-prompt <preset>` - Export preset to file\n- `/promptcode-ask-expert <question>` - AI consultation with code context\n\n## Workflow Examples\n\n### Discovery â†’ Context â†’ Expert\n```bash\n/promptcode-preset-list                    # Find existing presets\n/promptcode-preset-create auth system      # Or create focused preset\n/promptcode-ask-expert Why is login slow?  # Consult with context\n```\n\n### Direct CLI Usage\n```bash\npromptcode expert \"Review this\" --preset api --yes   # After cost approval\npromptcode generate -f \"src/**/*.ts\" -o prompt.txt   # Export for external use\n```\n\n## Cost Approval Protocol\n1. CLI estimates cost (threshold: $0.50)\n2. CC asks user ONCE for approval\n3. CC re-runs with `--yes` flag\n\n## API Keys Required\n```bash\nexport OPENAI_API_KEY=sk-...     # GPT/O3 models\nexport ANTHROPIC_API_KEY=sk-...  # Claude models\nexport GOOGLE_API_KEY=...        # Gemini models\nexport XAI_API_KEY=...           # Grok models\n```\n\nðŸ’¡ **Tip**: Create focused presets for better context and lower costs.\n<!-- PROMPTCODE-CLI-END -->",
  "promptcode-ask-expert.md": "---\nallowed-tools: Task, Read(/tmp/*), Read(/var/folders/*), Write(/tmp/*), Write(/var/folders/*), Edit(/tmp/*), Edit(/var/folders/*), Edit(.promptcode/presets/*), Bash, Bash(*)\ndescription: Consult AI expert for complex problems with code context - supports ensemble mode for multiple models\n---\n\nConsult an expert about: $ARGUMENTS\n\n## Instructions:\n\n1. Analyze the request in $ARGUMENTS:\n   - Extract the main question/problem\n   - Identify if code context would help (look for keywords about implementation, feature, code review, etc.)\n   - If the user mentions screenshots/images/mocks: ask for image file paths; plan to run with vision models using `--images` or `--allow-images`\n   - Check for multiple model requests (e.g., \"compare using gpt-5 and opus-4\", \"ask gpt-5, sonnet-4, and gemini\")\n   - Get available models dynamically: `promptcode expert --models --json` (parse the JSON for model list)\n   - If 2+ models detected â†’ use ensemble mode\n   - For single model: Use gpt-5.1 (default, updated from gpt-5) unless user explicitly specifies another model\n   - Vision-capable models: gpt-5/5.1 (+ mini/nano), sonnet/opus 4.x, gemini-3-pro/2.5, grok-4. Background mode is disabled when images are attached.\n\n2. Determine code context needs:\n   ```bash\n   promptcode preset list\n   ```\n   - Check if an existing preset matches the request (e.g., \"security\" â†’ look for security-related presets)\n   - If no suitable preset exists, create one:\n     ```bash\n     promptcode preset create {descriptive-name}\n     ```\n     Then edit `.promptcode/presets/{descriptive-name}.patterns` to add relevant file patterns.\n     Or use `--from-files` with specific patterns:\n     ```bash\n     promptcode preset create {descriptive-name} --from-files \"src/**/*.ts\" \"tests/**/*.test.ts\"\n     ```\n   - Verify the preset:\n     ```bash\n     promptcode preset info {preset-name}\n     ```\n\n3. Prepare consultation file for review:\n   - Set temp directory: `TMP=\"${TMPDIR:-/tmp}\"`\n   - Create unique files: `PROMPT_FILE=\"${TMP%/}/expert-consultation-$(date +%Y%m%d-%H%M%S)-$$.txt\"`\n   - Structure the file with:\n     ```markdown\n     # Expert Consultation\n     \n     ## Question\n     {user's question}\n     \n     ## Context\n     {any relevant context or background}\n     \n     ## Code Context\n     ```\n   - Append the code context using the preset:\n     ```bash\n     CODE_FILE=\"${TMP%/}/code-context-$(date +%Y%m%d-%H%M%S)-$$.txt\"\n     promptcode generate --preset \"{preset_name}\" --output \"$CODE_FILE\"\n     cat \"$CODE_FILE\" >> \"$PROMPT_FILE\"\n     ```\n\n4. Open consultation for user review:\n   ```bash\n   # Try cursor first, then code, then EDITOR, then xdg-open/open\n   if command -v cursor &> /dev/null; then\n     cursor \"$PROMPT_FILE\"\n   elif command -v code &> /dev/null; then\n     code \"$PROMPT_FILE\"\n   elif [ -n \"$EDITOR\" ]; then\n     \"$EDITOR\" \"$PROMPT_FILE\"\n   elif command -v xdg-open &> /dev/null; then\n     xdg-open \"$PROMPT_FILE\"\n   elif command -v open &> /dev/null; then\n     open \"$PROMPT_FILE\"\n   else\n     echo \"ðŸ“„ Consultation file created at: $PROMPT_FILE\"\n     echo \"No editor found. Please open the file manually to review.\"\n   fi\n   ```\n   \n5. Estimate cost and get approval:\n   ```bash\n   promptcode expert --prompt-file \"$PROMPT_FILE\" --model <model> --estimate-cost --json\n   ```\n   - Parse JSON for cost estimate using correct schema:\n     - Total cost: `.cost.total`\n     - Input tokens: `.tokens.input`\n   - Exit code: Always 0 for estimate (not 2 - that's for actual consultation requiring approval)\n   \n   **For single model:**\n   - Say: \"Ready to consult {model} using preset '{preset_name}' ({tokens} tokens, ~${cost}). Reply 'yes' to proceed.\"\n   \n   **For ensemble mode (multiple models):**\n   - Run --estimate-cost for each model in parallel\n   - Say: \"Ready for ensemble consultation with {models} ({total_tokens} tokens). Total: ${total_cost} ({model1}: ${cost1}, {model2}: ${cost2}). Reply 'yes' to proceed.\"\n   \n   **Important: Ask for approval ONLY ONCE - after showing cost estimate**\n\n6. Execute based on mode:\n\n   **Single Model Mode:**\n\n   **For fast models (gpt-5, sonnet-4, opus-4, gemini-2.5-pro, grok-4, etc.):**\n   Run in foreground:\n   ```bash\n   promptcode expert --prompt-file \"$PROMPT_FILE\" --model {model} --yes --json\n   ```\n\n   **For long-running models (gpt-5-pro - can take 10-120 minutes):**\n   Use the Task tool for non-blocking execution. The CLI will automatically switch to\n   OpenAI's background API for GPT-5 Pro, so no manual timeout wrapper is needed.\n\n   1. Create result file path using temp directory:\n      ```bash\n      TMP=\"${TMPDIR:-/tmp}\"\n      RESULT_FILE=\"${TMP%/}/expert-result-$(date +%Y%m%d-%H%M%S)-$$.json\"\n      ```\n\n   2. Inform the user:\n      ```\n      â³ Starting background consultation with {model}...\n         This may take a long time (10-120 min). Launching as autonomous Task...\n         Prompt file: $PROMPT_FILE\n         Results will be saved to: $RESULT_FILE\n         You can continue working - the Task will report back when complete.\n      ```\n\n   3. **Invoke the Task tool directly** with concrete paths and explicit tool usage:\n      ```\n      Task(\"Consult {model}: {short_question_summary}\", \"\"\"\nYou have access to: Bash, Read(/tmp/*), Read(/var/folders/*), Write(/tmp/*), Write(/var/folders/*).\n\nYour task: Run a long-running AI consultation (10-120 minutes) and report back with results.\n\n**Step 1: Run consultation and capture JSON to stdout**\nUse Bash tool to run:\npromptcode expert --prompt-file \"{absolute_path_to_PROMPT_FILE}\" --model {model} --yes --json > \"{absolute_path_to_RESULT_FILE}\"\nEXIT=$?\n\nNotes:\n- Replace {absolute_path_to_PROMPT_FILE} with actual path (e.g., /var/folders/.../expert-consultation-20251011-094021-48499.txt)\n- Replace {absolute_path_to_RESULT_FILE} with actual path (e.g., /var/folders/.../expert-result-20251011-094021-48499.json)\n- --json outputs structured JSON to stdout (NOT to --output file!)\n- Redirect stdout to capture JSON: > \"{absolute_path_to_RESULT_FILE}\"\n- Capture exit code in $EXIT variable\n\n**Step 2: Check exit code and classify status**\nUse Bash tool:\nif [ $EXIT -eq 0 ]; then\n  STATUS=\"SUCCESS\"\nelse\n  STATUS=\"FAILED\"\nfi\n\n**Step 3: Read and parse JSON result (only if SUCCESS)**\nIf STATUS is SUCCESS:\n- Use Read tool to read: {absolute_path_to_RESULT_FILE}\n- Parse the JSON using correct schema:\n  - Response text: .response\n  - Actual cost: .costBreakdown.actualTotal\n  - Input tokens: .usage.promptTokens\n  - Output tokens: .usage.completionTokens\n  - Response time: .responseTime (in seconds)\n\n**Step 4: Report back**\nReturn a structured report:\n- Status: SUCCESS | FAILED | TIMEOUT\n- Model: {model}\n- Cost: $X.XX (from .costBreakdown.actualTotal)\n- Tokens: X input, Y output (from .usage.promptTokens and .usage.completionTokens)\n- Response time: X.Xs (from .responseTime)\n- Summary: Brief summary of key insights from .response (2-3 sentences)\n- Result file: {absolute_path_to_RESULT_FILE}\n\n**On TIMEOUT or FAILED:**\n- Report the status and exit code\n- If result file exists, include any partial output\n- Surface key stderr lines from the CLI (timeouts show \"TimeoutError\")\n- No need for full error details - just status\n\nReturn all information to the main conversation so the user can see the results.\n\"\"\")\n      ```\n\n      **Important**: Replace all placeholder paths with actual absolute paths before invoking Task.\n\n   4. Return immediately so user can continue working\n   5. Task runs autonomously and persists across Claude Code sessions\n   6. When Task completes and reports back, summarize the findings for the user\n   \n   **Ensemble Mode (Parallel Execution):**\n\n   Create unique result files for each model:\n   ```bash\n   TMP=\"${TMPDIR:-/tmp}\"\n   RESULT_FILE_1=\"${TMP%/}/expert-{model1}-$(date +%Y%m%d-%H%M%S)-$$.json\"\n   RESULT_FILE_2=\"${TMP%/}/expert-{model2}-$(date +%Y%m%d-%H%M%S)-$$.json\"\n   SYNTHESIS_FILE=\"${TMP%/}/expert-synthesis-$(date +%Y%m%d-%H%M%S)-$$.md\"\n   # Add more for additional models (max 4)\n   ```\n\n   Inform the user:\n   ```\n   â³ Starting ensemble consultation with {model1}, {model2}...\n      This will run in parallel. Each model may take 1-10 minutes.\n      Launching parallel Tasks...\n      You can continue working - I'll synthesize results when both complete.\n   ```\n\n   **Launch parallel Tasks at top level (not nested):**\n\n   Invoke Task for model1:\n   ```\n   Task(\"Consult {model1}: {short_question}\", \"\"\"\nYou have access to: Bash, Read(/tmp/*), Read(/var/folders/*).\n\nRun consultation with timeout and capture JSON:\ntimeout 15m promptcode expert --prompt-file \"{absolute_path_to_PROMPT_FILE}\" --model {model1} --yes --json > \"{absolute_path_to_RESULT_FILE_1}\"\nEXIT=$?\n\nClassify status:\nif [ $EXIT -eq 0 ]; then STATUS=\"SUCCESS\"\nelif [ $EXIT -eq 124 ]; then STATUS=\"TIMEOUT\"\nelse STATUS=\"FAILED\"; fi\n\nIf SUCCESS, read JSON from {absolute_path_to_RESULT_FILE_1} and parse:\n- Response: .response\n- Cost: .costBreakdown.actualTotal\n- Tokens: .usage.promptTokens, .usage.completionTokens\n- Time: .responseTime\n\nReturn structured report:\n- Status: SUCCESS|TIMEOUT|FAILED\n- Model: {model1}\n- Cost: $X.XX\n- Tokens: X in, Y out\n- Time: X.Xs\n- File: {absolute_path_to_RESULT_FILE_1}\n\"\"\")\n   ```\n\n   Invoke Task for model2 (in same turn, runs in parallel):\n   ```\n   Task(\"Consult {model2}: {short_question}\", \"\"\"\nYou have access to: Bash, Read(/tmp/*), Read(/var/folders/*).\n\nRun consultation with timeout and capture JSON:\ntimeout 15m promptcode expert --prompt-file \"{absolute_path_to_PROMPT_FILE}\" --model {model2} --yes --json > \"{absolute_path_to_RESULT_FILE_2}\"\nEXIT=$?\n\nClassify status:\nif [ $EXIT -eq 0 ]; then STATUS=\"SUCCESS\"\nelif [ $EXIT -eq 124 ]; then STATUS=\"TIMEOUT\"\nelse STATUS=\"FAILED\"; fi\n\nIf SUCCESS, read JSON from {absolute_path_to_RESULT_FILE_2} and parse:\n- Response: .response\n- Cost: .costBreakdown.actualTotal\n- Tokens: .usage.promptTokens, .usage.completionTokens\n- Time: .responseTime\n\nReturn structured report:\n- Status: SUCCESS|TIMEOUT|FAILED\n- Model: {model2}\n- Cost: $X.XX\n- Tokens: X in, Y out\n- Time: X.Xs\n- File: {absolute_path_to_RESULT_FILE_2}\n\"\"\")\n   ```\n\n   **After both Tasks complete:**\n\n   1. Read both result JSON files using Read tool\n   2. Parse each using correct schema (.response, .costBreakdown.actualTotal, etc.)\n   3. Create synthesis report using Write tool at {absolute_path_to_SYNTHESIS_FILE}:\n\n   ```markdown\n   # Ensemble Expert Consultation Results\n\n   ## Question\n   {original_question}\n\n   ## Expert Responses\n\n   ### {Model1} - ${actual_cost}, {time}s\n   **Key Points:**\n   - [Extract 3-4 main insights from .response]\n\n   ### {Model2} - ${actual_cost}, {time}s\n   **Key Points:**\n   - [Extract 3-4 main insights from .response]\n\n   ## Synthesis\n\n   **Consensus Points:**\n   - [Where both models agree]\n\n   **Divergent Views:**\n   - {Model1}: [Unique insight]\n   - {Model2}: [Unique insight]\n\n   **ðŸ† WINNER: {model_name}**\n   Reason: [Clear, specific reason - e.g., \"More thorough analysis\", \"Better practical recommendations\"]\n\n   (Or: \"TIE - Both provided equally valuable but complementary insights\")\n\n   **Performance Summary:**\n   - Total Cost: ${total}\n   - {Model1}: ${cost1}, {time1}s\n   - {Model2}: ${cost2}, {time2}s\n   ```\n\n   4. Open synthesis file in Cursor if available\n   5. Report winner and key findings to user\n\n   **Important**:\n   - Replace all placeholder paths with actual absolute paths\n   - Invoke both Tasks in same turn for parallel execution\n   - Limit to maximum 4 models for practical reasons\n   - User has already approved costs via --yes flag\n\n7. Handle the response:\n\n   **Single Model Mode (Fast Models - Foreground):**\n   - Command returns JSON with structured output\n   - Parse JSON for cost, tokens, timing, and response\n   - Summarize key insights to user\n   - If result file was created, optionally open in Cursor/editor\n\n   **Single Model Mode (Long-Running - Background Task):**\n   - Task will report back when complete (happens in background)\n   - When Task returns, it provides structured summary\n   - Share the summary with user\n   - Open result file in Cursor if available\n\n   **Ensemble Mode (Background Task):**\n   - Parent Task orchestrates parallel consultations\n   - When Task completes, it provides:\n     - Synthesis file path\n     - Winner declaration with reasoning\n     - Cost and timing breakdown\n   - Share summary with user\n   - Open synthesis file in Cursor if available\n\n   **Error Handling:**\n   - If any model fails in ensemble mode, Task will continue with successful ones and report which failed\n   - If API key missing:\n     ```\n     To use expert consultation, set your OpenAI API key:\n     export OPENAI_API_KEY=sk-...\n     Get your key from: https://platform.openai.com/api-keys\n     ```\n   - For other errors: Report exact error message from Task or CLI\n   - Timeout errors: Report which model timed out (exceeded 15 minutes)\n\n## Important:\n- **Always use presets** - either existing or create new ones for code context\n- **Single approval flow**: Estimate cost â†’ Ask user ONCE â†’ Execute with --yes\n- **Show the preset name** to the user so they know what context is being used\n- **Default model is gpt-5.1** - use this unless user explicitly requests another model\n- For ensemble mode: limit to maximum 4 models\n- NEVER automatically add --yes without user approval\n- **JSON output modes**:\n  - Fast models (foreground): `--json` outputs to stdout, parse directly\n  - Long-running (Task): Redirect stdout to file: `--json > \"$FILE\"`, then Read file\n  - Never use `--output` with `--json` (--output writes plain text, not JSON!)\n- **JSON schema differences**:\n  - Estimate mode: Use `.cost.total`, `.tokens.input`\n  - Actual result: Use `.response`, `.costBreakdown.actualTotal`, `.usage.promptTokens/.completionTokens`, `.responseTime`\n- **Timeout protection**: All consultations use `timeout 15m` command, exit code 124 = timeout\n- **Task-based execution**: For long-running models (gpt-5-pro, o3-pro), use Task tool for non-blocking execution\n  - Pass concrete absolute file paths to Task (not shell variables like $PROMPT_FILE)\n  - Explicitly state tool usage (Bash, Read, Write) in Task prompt\n  - Capture exit code: `EXIT=$?` after command, then classify: 0=SUCCESS, 124=TIMEOUT, else=FAILED\n  - Task provides true non-blocking execution that persists across sessions\n  - Task will report back when complete\n- **Fast vs slow models**: Fast models (gpt-5, sonnet-4, opus-4, etc) take <30s and run in foreground. Long-running models (gpt-5-pro, o3-pro) take 1-10 minutes and should use Task tool\n- **Ensemble execution**: Invoke multiple Tasks at top level in same turn for parallel execution (not nested Tasks!)\n  - Each Task runs one model consultation\n  - After all Tasks complete, synthesize results yourself\n  - Declare clear winner with reasoning\n- **File paths**: Always use `${TMPDIR:-/tmp}` pattern for cross-platform temp directories (macOS uses /var/folders, Linux uses /tmp)\n- Reasoning effort defaults to 'high' (set in CLI) - no need to specify\n",
  "promptcode-preset-create.md": "---\nallowed-tools: Bash(promptcode preset create:*), Bash(promptcode preset info:*), Glob(**/*), Grep, Write(.promptcode/presets/*.patterns), Read(/tmp/*), Write(/tmp/*), Bash, Bash(*)\ndescription: Create a promptcode preset from description\n---\n\nCreate a promptcode preset for: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the description to understand what code to capture:\n   - Look for keywords like package names, features, components, integrations\n   - Identify if it's Python, TypeScript, or mixed code\n   - Determine the scope (single package, cross-package feature, etc.)\n\n2. Research the codebase structure:\n   - Use Glob to explore relevant directories\n   - Use Grep to find related files if needed\n   - Identify the main code locations and any related tests/docs\n\n3. Generate a descriptive preset name:\n   - Use kebab-case (e.g., \"auth-system\", \"microlearning-utils\")\n   - Keep it concise but descriptive\n\n4. Create the preset (automatically optimized from concrete files):\n   ```bash\n   # When you identify specific files, always use --from-files for smart optimization\n   promptcode preset create \"{preset_name}\" --from-files {file-globs...}\n   # default optimization-level is \"balanced\"\n   # to control: --optimization-level minimal|balanced|aggressive\n   ```\n   This creates `.promptcode/presets/{preset_name}.patterns` with optimized patterns.\n\n5. Edit the preset file to add patterns (if needed):\n   - Start with a header comment explaining what the preset captures\n   - Add inclusion patterns for the main code\n   - Add patterns for related tests and documentation\n   - Include common exclusion patterns:\n     - `!**/__pycache__/**`\n     - `!**/*.pyc`\n     - `!**/node_modules/**`\n     - `!**/dist/**`\n     - `!**/build/**`\n\n6. Test and report results:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n   Report the file count and estimated tokens.\n\n## Common Pattern Examples:\n- Python package: `python/cogflows-py/packages/{package}/src/**/*.py`\n- TypeScript component: `ts/next/{site}/components/{component}/**/*.{ts,tsx}`\n- Cross-package feature: Multiple specific paths\n- Tests: `python/cogflows-py/packages/{package}/tests/**/*.py`\n- Documentation: `**/{feature}/**/*.md`",
  "promptcode-preset-info.md": "---\nallowed-tools: Bash(promptcode preset info:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*), Bash, Bash(*)\ndescription: Show detailed information about a promptcode preset\n---\n\nShow detailed information about promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the arguments to identify the preset:\n   - If exact preset name provided (e.g., \"functional-framework\"), use it directly\n   - If description provided, infer the best matching preset:\n     - Run `promptcode preset list` to see available presets\n     - Read header comments from preset files in `.promptcode/presets/` if needed\n     - Match based on keywords and context\n     - Choose the most relevant preset\n\n2. Run the promptcode info command with the determined preset name:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n\n3. If a preset was inferred from description, explain which preset was chosen and why.\n\nThe output will show:\n- Preset name and path\n- Description from header comments\n- File count and token statistics\n- Pattern details\n- Sample files included\n- Usage instructions",
  "promptcode-preset-list.md": "---\nallowed-tools: Bash(promptcode preset list:*)\ndescription: List all available promptcode presets with pattern counts\n---\n\nList all available promptcode presets.\n\nRun the command:\n```bash\npromptcode preset list\n```\n\nThis will display all available presets with their pattern counts. Use the preset names with other promptcode commands to work with specific code contexts.",
  "promptcode-preset-to-prompt.md": "---\nallowed-tools: Bash(promptcode generate:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*), Read(/tmp/*), Write(/tmp/*), Bash, Bash(*)\ndescription: Generate AI-ready prompt file from a promptcode preset with optional instructions\n---\n\nGenerate prompt file from promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse arguments to understand what the user wants:\n   - Extract preset name or description (may be quoted)\n   - Detect explicit instructions delimiter:\n     - If arguments contain \" -- \" (space, two dashes, space), everything after it is the instructions (preserve verbatim)\n     - Else, look for explicit -i or --instructions value; if provided, use it verbatim as instructions\n   - Keep remaining tokens for output path and/or fallback instructions\n\n2. If inferring from description:\n   - Run `promptcode preset list` to see available presets\n   - Read header comments from `.promptcode/presets/*.patterns` files if needed\n   - Match based on keywords and context\n   - Choose the most relevant preset\n\n3. Determine output path and instructions (fallback):\n   - Resolve output path using existing keywords:\n     - \"to [path]\" means explicit file path (or folder if ends with '/')\n     - \"in [folder]\" means folder\n     - \"as [filename]\" means filename in /tmp unless combined with \"in\"\n   - After removing path-spec tokens, if instructions were NOT set via delimiter or -i/--instructions and there are remaining tokens:\n     - Treat remaining tokens (in original order) as the instructions (fallback mode)\n   - Default output path: `/tmp/promptcode-{preset-name}-{timestamp}.txt` where timestamp is YYYYMMDD-HHMMSS\n\n4. Generate the prompt file:\n   - If instructions are present, determine how to pass them:\n     - For very long instructions (>500 chars): Save to temp file and use --instructions-file\n     - For shorter instructions: Pass directly using --instructions (alias -i)\n   - IMPORTANT: Shell-escape ALL parameters:\n     - Always wrap ALL strings (preset_name, output_path, instructions) in single quotes\n     - Replace any single quote ' inside ANY parameter with '\\'' (close-quote, escaped quote, reopen-quote)\n     - This prevents command injection and ensures security\n   - Command forms:\n\n     ```bash\n     # Without instructions:\n     promptcode generate --preset '{preset_name}' --output '{output_path}'\n     \n     # With short instructions (all values properly escaped with ' replaced by '\\''):\n     promptcode generate --preset '{preset_name}' --output '{output_path}' --instructions '{INSTR_ESC}'\n     \n     # With long instructions (saved to temp file):\n     INSTR_FILE=\"${TMP%/}/instructions-$(date +%Y%m%d-%H%M%S)-$$.txt\"\n     echo '{instructions_content}' > \"$INSTR_FILE\"\n     promptcode generate --preset '{preset_name}' --output '{output_path}' --instructions-file \"$INSTR_FILE\"\n     ```\n\n5. Report results:\n   - Which preset was used (especially important if inferred)\n   - Full path to the output file\n   - Whether instructions were included (show first ~120 chars for confirmation)\n   - Token count and number of files included\n   - Suggest next steps (e.g., \"You can now open this file in your editor\")\n\n## Examples of how users might call this\n\n- `/promptcode-preset-to-prompt functional-framework`\n- `/promptcode-preset-to-prompt functional-framework -- Review for security and performance`\n- `/promptcode-preset-to-prompt functional-framework to ~/Desktop/analysis.txt -- How to migrate to TS 5.6?`\n- `/promptcode-preset-to-prompt functional-framework in ~/Desktop as analysis.txt -- Identify dead code`\n- `/promptcode-preset-to-prompt \"functional framework\" -i \"Focus on memory leaks in parsers\"`\n- `/promptcode-preset-to-prompt microlearning analysis to ~/Desktop/`\n- `/promptcode-preset-to-prompt the functional code as analysis.txt`\n",
  "promptcode-ask-expert.mdc": "---\ndescription: Ask AI expert questions with code context using promptcode\nalwaysApply: false\n---\n\nAsk AI expert questions with code context using promptcode.\n\nWhen asked to consult an expert or analyze code:\n```bash\npromptcode expert \"<question>\" --preset <preset-name>\n```\n\nOr with specific files:\n```bash\npromptcode expert \"<question>\" -f \"src/**/*.ts\"\n```\n\nOptions:\n- `--model <name>`: Choose AI model (default: gpt-5.1; others: gpt-5, opus-4, sonnet-4.5, gemini-3-pro/2.5, grok-4, o3, o3-pro, gpt-5-pro)\n- `--web-search`: Enable web search for current information\n- `--images <globs>` / `--allow-images`: Attach images (vision models only: gpt-5/5.1, sonnet-4.5, opus-4.x, gemini-3-pro/2.5, grok-4)\n- `--yes`: Skip cost confirmation prompts\n- `--output <file>`: Save response to file (required for background execution)\n- `--json`: Return structured JSON output (recommended for parsing)\n\nNote: The default model is **gpt-5.1** unless explicitly specified otherwise.\n\n## Long-Running Models (gpt-5-pro only)\n\nGPT-5 Pro can run for 10-120 minutes. The PromptCode CLI automatically switches to\nOpenAI's background API for this model, so the wrapper script just needs to launch the\nCLI in the background and capture its JSON output (no manual timeout).\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Cross-platform mktemp (BSD mktemp on macOS behaves differently)\nTMP=\"${TMPDIR:-/tmp}\"\nRESULT_FILE=\"${TMP%/}/expert-result-$(date +%Y%m%d-%H%M%S)-$$.json\"\nLOG_FILE=\"${TMP%/}/expert-log-$(date +%Y%m%d-%H%M%S)-$$.txt\"\nSTATUS_FILE=\"${TMP%/}/expert-status-$(date +%Y%m%d-%H%M%S)-$$.json\"\n\n# Prepare the consultation\nQUESTION=\"<your question here>\"\nMODEL=\"gpt-5-pro\"  # CLI auto-enables background mode for this model\n\n# Helper function for atomic status writes\nwrite_status() {\n  printf '%s' \"$1\" > \"${STATUS_FILE}.tmp\" && mv \"${STATUS_FILE}.tmp\" \"$STATUS_FILE\"\n}\n\n# Save initial status\nwrite_status \"{\n  \\\"status\\\": \\\"starting\\\",\n  \\\"model\\\": \\\"$MODEL\\\",\n  \\\"started\\\": \\\"$(date +%s)\\\",\n  \\\"result\\\": \\\"$RESULT_FILE\\\",\n  \\\"log\\\": \\\"$LOG_FILE\\\"\n}\"\n\necho \"â³ Starting background consultation with $MODEL...\"\necho \"   Results: $RESULT_FILE\"\necho \"   Log: $LOG_FILE\"\necho \"   Status: $STATUS_FILE\"\n\n# Run in background with nohup and proper quoting\n# Note: Use bash -lc to ensure login shell environment (API keys loaded)\nnohup bash -lc \"\n  set -euo pipefail\n\n  # Helper for atomic writes\n  write_status() {\n    printf '%s' \"$1\" > \"${STATUS_FILE}.tmp\" && mv \"${STATUS_FILE}.tmp\" \"$STATUS_FILE\"\n  }\n\n  # Run consultation and capture JSON to stdout (CLI manages background execution)\n  promptcode expert \"$QUESTION\" \\\n    --model \"$MODEL\" \\\n    --yes \\\n    --json > \"$RESULT_FILE\" 2>> \"$LOG_FILE\"\n\n  EXIT_CODE=$?\n\n  # Update status based on exit code (atomic write)\n  if [ $EXIT_CODE -eq 0 ]; then\n    write_status \"{\\\"status\\\":\\\"success\\\",\\\"exitCode\\\":0,\\\"completed\\\":\\\"$(date +%s)\\\"}\"\n  else\n    write_status \"{\\\"status\\\":\\\"failed\\\",\\\"exitCode\\\":$EXIT_CODE,\\\"completed\\\":\\\"$(date +%s)\\\"}\"\n  fi\n\" >> \"$LOG_FILE\" 2>&1 &\n\nBG_PID=$!\ndisown \"$BG_PID\" 2>/dev/null || true\n\n# Update status with PID (atomic write)\nwrite_status \"{\n  \\\"status\\\": \\\"running\\\",\n  \\\"pid\\\": $BG_PID,\n  \\\"model\\\": \\\"$MODEL\\\",\n  \\\"started\\\": \\\"$(date +%s)\\\",\n  \\\"result\\\": \\\"$RESULT_FILE\\\",\n  \\\"log\\\": \\\"$LOG_FILE\\\"\n}\"\n\necho \"\"\necho \"âœ“ Background consultation launched (PID: $BG_PID)\"\necho \"\"\necho \"Monitor progress:\"\necho \"  tail -f \"$LOG_FILE\"\"\necho \"\"\necho \"Check status:\"\necho \"  kill -0 $BG_PID 2>/dev/null && echo 'Still running' || echo 'Complete or failed'\"\nif command -v jq >/dev/null 2>&1; then\n  echo \"  cat \"$STATUS_FILE\" | jq .\"\nelse\n  echo \"  cat \"$STATUS_FILE\"  # Install jq for pretty output: brew install jq\"\nfi\necho \"\"\necho \"View results when complete:\"\nif command -v jq >/dev/null 2>&1; then\n  echo \"  cat \"$RESULT_FILE\" | jq .\"\n  echo \"  cat \"$RESULT_FILE\" | jq -r '.response'  # Just the response text\"\nelse\n  echo \"  cat \"$RESULT_FILE\"  # Install jq for pretty output: brew install jq\"\nfi\necho \"\"\necho \"Note: PromptCode CLI enforces internal 120-minute safeguards for GPT-5 Pro\"\n```\n**Monitoring a running consultation:**\n\n```bash\n# Check if still running (replace PID)\nkill -0 <PID> 2>/dev/null && echo \"Running\" || echo \"Stopped\"\n\n# View live progress\ntail -f /var/folders/.../expert-log-*.txt\n\n# Check final status\nif command -v jq >/dev/null 2>&1; then\n  cat /var/folders/.../expert-status-*.json | jq .\nelse\n  cat /var/folders/.../expert-status-*.json\nfi\n\n# Read result when complete (actual response in .response field)\nif command -v jq >/dev/null 2>&1; then\n  cat /var/folders/.../expert-result-*.json | jq -r '.response'\n  cat /var/folders/.../expert-result-*.json | jq '.costBreakdown.actualTotal'  # Actual cost\nelse\n  cat /var/folders/.../expert-result-*.json\nfi\n```\n\n## API Keys\n\nThis requires API keys to be set:\n- `OPENAI_API_KEY` for O3/GPT models\n- `ANTHROPIC_API_KEY` for Claude models\n- `GOOGLE_API_KEY` for Gemini models\n- `XAI_API_KEY` for Grok models\n\nFor expensive operations (>$0.50), always inform the user of the cost before proceeding.\n",
  "promptcode-preset-create.mdc": "---\ndescription: Create a new promptcode preset from a description\nalwaysApply: false\n---\n\nCreate a new promptcode preset based on a description.\n\n## Basic Usage\n\nWhen asked to create a preset, run:\n```bash\npromptcode preset create <preset-name>\n```\n\nThis creates a basic preset template that you can edit manually.\n\n## Smart Creation with Auto-Optimization\n\nFor better results, create presets from existing files:\n```bash\n# Automatically optimizes patterns from concrete files\npromptcode preset create <preset-name> --from-files \"src/api/**/*.ts\" \"src/utils/**/*.ts\"\n\n# Control optimization level (default: balanced)\npromptcode preset create <preset-name> --from-files \"**/*.ts\" --optimization-level aggressive\n```\n\nOptimization levels:\n- **minimal**: Light optimization, preserves most patterns\n- **balanced**: Default, good balance of pattern reduction\n- **aggressive**: Maximum reduction, fewer patterns\n\n## What It Does\n\n1. Creates preset file in `.promptcode/presets/<preset-name>.patterns`\n2. If using `--from-files`: Analyzes files and generates optimized patterns\n3. Makes preset available for all promptcode commands\n\n## Optimizing Existing Presets\n\nYou can also optimize presets after creation:\n```bash\npromptcode preset optimize <preset-name>           # Preview changes\npromptcode preset optimize <preset-name> --write   # Apply optimization\n```\n\nExample: `promptcode preset create api-endpoints --from-files \"src/api/**/*.ts\"`",
  "promptcode-preset-info.mdc": "---\ndescription: Show details and token count for a specific promptcode preset\nalwaysApply: false\n---\n\nShow details and token count for a specific promptcode preset.\n\nWhen asked about a preset, run:\n```bash\npromptcode preset info <preset-name>\n```\n\nThis displays:\n- File patterns included in the preset\n- Total file count\n- Token count estimate\n- List of matched files\n- Usage examples\n\n## Optimizing Presets\n\nIf the preset has too many patterns or includes too many files, optimize it:\n```bash\npromptcode preset optimize <preset-name>           # Preview changes\npromptcode preset optimize <preset-name> --write   # Apply optimization\n```\n\nOptimization intelligently reduces pattern count while maintaining file coverage.\n\nExample: `promptcode preset info backend`",
  "promptcode-preset-list.mdc": "---\ndescription: List all available promptcode presets with pattern counts\nalwaysApply: false\n---\n\nList all available promptcode presets.\n\nRun the command:\n```bash\npromptcode preset list\n```\n\nThis will display all available presets with their pattern counts. Use the preset names with other promptcode commands to work with specific code contexts.",
  "promptcode-preset-to-prompt.mdc": "---\ndescription: Export a promptcode preset to a file for sharing or archiving\nalwaysApply: false\n---\n\nExport a promptcode preset to a file.\n\nWhen asked to export or save a preset:\n```bash\npromptcode generate --preset <preset-name> --output <output-file>\n```\n\nThis generates a complete prompt with:\n- All file contents from the preset\n- Structured XML-like formatting\n- Token count information\n\nExamples:\n- Export to file: `promptcode generate --preset backend --output backend-context.md`\n- Export with instructions: `promptcode generate --preset api --instructions \"Review for security issues\" --output security-review.md`",
  "promptcode-usage.mdc": "---\ndescription: How to use PromptCode CLI effectively from Cursor\nalwaysApply: false\n---\n\n# PromptCode CLI Usage Guide\n\nPromptCode is a CLI tool that helps generate AI-ready prompts from codebases. When working with PromptCode:\n\n## Core Commands\n\n### Working with Presets\n- `promptcode preset list` - Show all available presets\n- `promptcode preset info <name>` - Get details about a specific preset\n- `promptcode preset create <name>` - Create a new preset\n\n### Generating Prompts\n- `promptcode generate --preset <preset>` - Generate prompt using a preset\n- `promptcode generate -f \"pattern\"` - Generate with file patterns\n- `promptcode generate --output output.md` - Save to file\n\n### Expert Consultation\n- `promptcode expert \"question\"` - Ask AI experts with code context\n- Requires API keys: OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, or XAI_API_KEY\n\n## Pseudo-Commands\n\nYou can use these shortcuts that I'll interpret (matching Claude Code commands):\n- `/promptcode-preset-list` â†’ `promptcode preset list`\n- `/promptcode-preset-info <name>` â†’ `promptcode preset info <name>`\n- `/promptcode-preset-create <name>` â†’ `promptcode preset create <name>`\n- `/promptcode-ask-expert \"question\"` â†’ `promptcode expert \"question\"`\n- `/promptcode-preset-to-prompt <preset>` â†’ `promptcode generate --preset <preset>`\n\n## Important Guidelines\n\n1. **Always request approval** before running commands that modify files\n2. **Check for API keys** when using expert mode\n3. **Parse and summarize** CLI output clearly for the user\n4. **Cost awareness**: For expert consultations over $0.50, always inform the user and get approval\n5. **Use presets** to manage common file pattern groups efficiently\n\n## Common Workflows\n\n### Analyzing a Feature\n1. Create or identify relevant preset: `promptcode preset create feature-name`\n2. Check what it includes: `promptcode preset info feature-name`\n3. Generate context: `promptcode generate --preset feature-name`\n\n### Getting AI Analysis\n1. Ensure API key is set (check environment)\n2. Run expert with context: `promptcode expert \"your question\" --preset <name>`\n3. For expensive models, get user approval first\n\nWhen in doubt, use `promptcode --help` or `promptcode <command> --help` for detailed usage information."
};

export function getEmbeddedTemplates(): Record<string, string> {
  return EMBEDDED_TEMPLATES;
}

export function hasEmbeddedTemplates(): boolean {
  return Object.keys(EMBEDDED_TEMPLATES).length > 0;
}
