/**
 * Embedded templates for compiled binaries.
 * This file is auto-generated during build - DO NOT EDIT MANUALLY.
 * Generated at: 2025-08-10T11:20:37.302Z
 */

// Template contents embedded at build time
const EMBEDDED_TEMPLATES: Record<string, string> = {
  "CLAUDE.md.template": "<!-- PROMPTCODE-CLI-START -->\n# PromptCode CLI\n\nGenerate AI-ready prompts from your codebase. The CLI is designed to be AI-friendly with clear commands and outputs.\n\n## Quick Start\n\n```bash\n# Generate a prompt with specific files\npromptcode generate -f src/api/handler.ts src/utils/*.ts\n\n# Ask AI experts questions with code context\npromptcode expert \"Why is this slow?\" -f src/api/handler.ts\n\n# Use presets for common file patterns\npromptcode preset list                    # See available presets\npromptcode preset info <name>             # Show preset details & token count\npromptcode generate --preset <preset-name>  # Generate using preset\n```\n\n## Working with Presets\n\nPresets are reusable file patterns stored in `.promptcode/presets/*.patterns`:\n\n```bash\n# Create a new preset\npromptcode preset create api-endpoints\n\n# Edit the preset file to add patterns\n# Then use it:\npromptcode generate --preset api-endpoints\n```\n\n## Common Workflows for AI Agents\n\n### 1. Discovering Code Structure\n```bash\n# List all presets to understand project organization\npromptcode preset list\n\n# Inspect a preset to see what files it includes\npromptcode preset info functional-utils\n```\n\n### 2. Creating Focused Presets\nWhen asked to analyze specific features:\n1. Create a descriptive preset: `promptcode preset create feature-name`\n2. Edit `.promptcode/presets/feature-name.patterns` with relevant patterns\n3. Use `promptcode preset info feature-name` to verify file selection\n4. Generate output: `promptcode generate --preset feature-name`\n\n### 3. Analyzing Code\n```bash\n# Generate prompt with specific concern\npromptcode generate -f \"src/**/*.ts\" --instructions \"Find performance bottlenecks\"\n\n# Or use expert mode for direct AI analysis\npromptcode expert \"Review this code for security issues\" -f \"src/api/**/*.ts\"\n```\n\n## Tips for AI Agents\n\n1. **Always check token counts** - Use `promptcode preset info` to see total tokens before generating\n2. **Be specific with patterns** - Use `src/api/*.ts` not `**/*.ts` to avoid huge contexts\n3. **Leverage existing presets** - Check `promptcode preset list` before creating new ones\n4. **Use descriptive preset names** - `auth-system` not `preset1`\n\n## Important: Cost Approval for AI Agents\n\nThe `expert` command includes built-in cost protection that requires approval for expensive operations (over $0.50 or using premium models). The CLI will automatically handle this in different environments:\n\n**In Interactive Mode (Terminal):**\n- The CLI will prompt the user directly for approval\n- Shows cost breakdown and waits for yes/no response\n\n**In Non-Interactive Mode (Claude Code, CI/CD):**\n```bash\n# Without approval flags, expensive operations will be blocked:\npromptcode expert \"Complex analysis\" --model o3-pro\n# Output: \"⚠️ Cost approval required for expensive operation (~$X.XX)\"\n#         \"Non-interactive environment detected.\"\n#         \"Use --yes to proceed with approval...\"\n```\n\n**AI Agent Approval Protocol:**\n1. **When you see \"Cost approval required\"**, STOP immediately\n2. **Inform the user**: \"This operation will cost approximately $X.XX. Do you want to proceed?\"\n3. **Wait for explicit user confirmation** (yes/no)\n4. **If approved**, re-run the command with `--yes` flag:\n   ```bash\n   promptcode expert \"Complex analysis\" --model o3-pro --yes\n   ```\n5. **If declined**, inform the user the operation was cancelled\n\n**Important Guidelines for AI Agents:**\n- **NEVER** automatically add `--yes` without explicit user consent\n- **ALWAYS** show the cost estimate before asking for approval\n- The `--yes` flag means \"I have user approval for this specific operation\"\n- The `--yes` flag can be used to auto-approve operations after user consent\n- Default to conservative behavior - when in doubt, ask for approval\n\n**Cost Information:**\n- Expensive models: o3-pro\n- Threshold: Operations over $0.50 require approval\n- The CLI shows detailed cost breakdowns before execution\n\n## Claude Code Integration\n\nWhen you run `promptcode cc`, it installs a custom command for Claude:\n- **`.claude/commands/expert-consultation.md`** - Allows Claude to properly use the expert command with cost approval\n\nThis command helps Claude:\n1. Check available presets and select appropriate context\n2. Handle cost approval properly (never auto-approving expensive operations)\n3. Use the correct model based on your request (o3 vs o3-pro)\n4. Parse and present results effectively\n\nTo use it in Claude, simply ask: \"Consult an expert about [your question]\"\n\n## Configuration\n\nAPI keys must be set via environment variables:\n```bash\nexport OPENAI_API_KEY=sk-...\nexport ANTHROPIC_API_KEY=sk-ant-...\nexport GOOGLE_API_KEY=...            # or GEMINI_API_KEY\nexport XAI_API_KEY=...                # or GROK_API_KEY\n```\n\n<details>\n<summary>⚠️ Troubleshooting</summary>\n\n• **Command not found** – The CLI auto-installs to `~/.local/bin`. Ensure it's in PATH  \n• **Missing API key** – Set via environment variable as shown above  \n• **Context too large** – Use more specific file patterns or create focused presets\n• **Preset not found** – Check `.promptcode/presets/` directory exists\n</details>\n<!-- PROMPTCODE-CLI-END -->",
  "expert-consultation.md": "---\nallowed-tools: Bash(promptcode expert:*), Bash(promptcode preset:*), Bash(open -a Cursor:*), Read(/tmp/expert-*:*)\ndescription: Consult OpenAI o3/o3-pro expert for complex problems with code context\n---\nConsult an expert about: $ARGUMENTS\n\nInstructions:\n1. Analyze the request in $ARGUMENTS to understand:\n   - The main question/problem\n   - Whether code context would help\n\n2. If code context would be helpful:\n   ```bash\n   promptcode preset list  # See available presets\n   ```\n   \n   Choose relevant preset(s) or create specific file patterns.\n\n3. Ask the expert with appropriate context:\n   ```bash\n   # With preset:\n   promptcode expert \"YOUR_CLEAR_QUESTION\" --preset <preset-name> --model <model>\n   \n   # With specific files:\n   promptcode expert \"YOUR_CLEAR_QUESTION\" -f \"src/**/*.ts\" --model <model>\n   \n   # Without context (general question):\n   promptcode expert \"YOUR_CLEAR_QUESTION\" --model <model>\n   ```\n   \n   The CLI will show estimated cost and ask for confirmation if:\n   - Cost exceeds $0.50\n   - Using a \"pro\" model\n   \n   IMPORTANT: If you see \"Non-interactive environment detected\":\n   - DO NOT automatically add --yes or --no-confirm\n   - STOP and inform the user about the cost\n   - Ask: \"This will cost approximately $X.XX. Do you want to proceed?\"\n   - Only proceed with --yes after user explicitly approves\n   \n   Note: --no-confirm is an auto-accept mode for users who want to bypass\n   all confirmations. AI agents should use --yes after explicit approval.\n   \n   Model options:\n   - `o3` - Standard O3 model ($2/$8 per million tokens)\n   - `o3-pro` - O3 Pro for complex tasks ($20/$80 per million tokens)\n   - If question mentions \"o3-pro\" or \"o3 pro\", use `--model o3-pro`\n   - Otherwise default to `--model o3`\n\n4. If output file was specified, open it:\n   ```bash\n   promptcode expert \"...\" --output response.md\n   open -a Cursor response.md  # or read the file\n   ```\n\n5. Parse the response:\n   - If successful: Summarize key insights\n   - If API key missing: Tell user to set environment variable:\n     ```bash\n     export OPENAI_API_KEY=sk-...\n     # Or for other providers:\n     export ANTHROPIC_API_KEY=sk-ant-...\n     export GOOGLE_API_KEY=...\n     export XAI_API_KEY=...\n     ```\n   - For other errors: Report exact error message\n\nIMPORTANT: \n- Always include relevant code context when asking about specific functionality\n- Be clear and specific in your questions\n- Choose o3-pro only for genuinely complex tasks requiring deep reasoning"
};

export function getEmbeddedTemplates(): Record<string, string> {
  return EMBEDDED_TEMPLATES;
}

export function hasEmbeddedTemplates(): boolean {
  return Object.keys(EMBEDDED_TEMPLATES).length > 0;
}
