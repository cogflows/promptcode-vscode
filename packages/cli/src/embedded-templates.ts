/**
 * Embedded templates for compiled binaries.
 * This file is auto-generated during build - DO NOT EDIT MANUALLY.
 * Generated at: 2025-08-11T08:49:36.121Z
 */

// Template contents embedded at build time
const EMBEDDED_TEMPLATES: Record<string, string> = {
  "CLAUDE.md.template": "<!-- PROMPTCODE-CLI-START -->\n# PromptCode CLI Integration\n\nThis project has PromptCode CLI integrated for AI-assisted code analysis. The CLI provides structured access to the codebase through presets and intelligent commands.\n\n## Available Claude Commands\n\nThe following commands are available to help you work with this codebase:\n\n- `/promptcode-preset-list` - List all available code presets\n- `/promptcode-preset-info <name>` - Show details and token count for a preset\n- `/promptcode-preset-create <description>` - Create a new preset from description\n- `/promptcode-preset-to-prompt <preset>` - Export preset to a file\n- `/promptcode-ask-expert <question>` - Consult AI expert with code context\n\n## Quick Examples\n\n```bash\n# See what presets are available\n/promptcode-preset-list\n\n# Get details about a specific preset\n/promptcode-preset-info auth-system\n\n# Create a preset for a feature\n/promptcode-preset-create authentication and authorization system\n\n# Ask an expert about the code\n/promptcode-ask-expert How does the authentication flow work?\n```\n\n## Direct CLI Usage\n\nFor simple operations, you can also use the CLI directly:\n\n```bash\n# Generate a prompt from files\npromptcode generate -f \"src/**/*.ts\" -o analysis.txt\n\n# Quick expert consultation (requires API key)\npromptcode expert \"Find security issues\" --preset api --yes\n\n# View preset information with JSON output\npromptcode preset info backend --json\n```\n\n## Configuration\n\nSet API keys via environment variables for expert consultations:\n```bash\nexport OPENAI_API_KEY=sk-...      # For O3/O3-pro models\nexport ANTHROPIC_API_KEY=sk-...   # For Claude models\nexport GOOGLE_API_KEY=...         # For Gemini models\nexport XAI_API_KEY=...            # For Grok models\n```\n\n## Cost Protection\n\nThe expert command has built-in cost protection:\n- Operations over $0.50 require explicit approval\n- Premium models (e.g., o3-pro) always require confirmation\n- Use `--yes` flag only after getting user approval\n\n<details>\n<summary>⚠️ Troubleshooting</summary>\n\n• **Command not found** – The CLI auto-installs to `~/.local/bin`. Ensure it's in PATH  \n• **Missing API key** – Set environment variables as shown above  \n• **Context too large** – Use more specific file patterns or focused presets  \n• **Preset not found** – Check `.promptcode/presets/` directory exists\n</details>\n<!-- PROMPTCODE-CLI-END -->",
  "promptcode-ask-expert.md": "---\nallowed-tools: Bash(promptcode expert:*), Bash(promptcode preset list:*), Bash(promptcode generate:*), Bash(open -a Cursor:*), Read(/tmp/expert-*:*), Write(/tmp/expert-consultation-*.md), Task\ndescription: Consult AI expert (O3/O3-pro) for complex problems with code context - supports ensemble mode for multiple models\n---\n\nConsult an expert about: $ARGUMENTS\n\n## Instructions:\n\n1. Analyze the request in $ARGUMENTS:\n   - Extract the main question/problem\n   - Identify if code context would help (look for keywords matching our presets)\n   - Check for multiple model requests (e.g., \"compare using o3 and gpt-5\", \"ask o3, gpt-5, and gemini\")\n   - Available models from our MODELS list: o3, o3-pro, o3-mini, gpt-5, gpt-5-mini, gpt-5-nano, sonnet-4, opus-4, gemini-2.5-pro, gemini-2.5-flash, grok-4\n   - If 2+ models detected → use ensemble mode\n   - For single model: determine preference (if user mentions \"o3-pro\" or \"o3 pro\", use o3-pro)\n\n2. If code context needed, list available presets:\n   ```bash\n   promptcode preset list\n   ```\n   Choose relevant preset(s) based on the question.\n\n3. Prepare consultation file for review:\n   - Create a consultation file at `/tmp/expert-consultation-{timestamp}.md`\n   - Structure the file with:\n     ```markdown\n     # Expert Consultation\n     \n     ## Question\n     {user's question}\n     \n     ## Context\n     {any relevant context or background}\n     ```\n   - If a preset would help, append the code context:\n     ```bash\n     echo -e \"\\n## Code Context\\n\" >> \"/tmp/expert-consultation-{timestamp}.md\"\n     promptcode generate --preset \"{preset_name}\" >> \"/tmp/expert-consultation-{timestamp}.md\"\n     ```\n\n4. Open consultation for user review (if Cursor is available):\n   ```bash\n   open -a Cursor \"/tmp/expert-consultation-{timestamp}.md\"\n   ```\n   \n5. Estimate cost and get approval:\n   - Model costs (from our pricing):\n     - O3: $2/$8 per million tokens (input/output)\n     - O3-pro: $20/$80 per million tokens (input/output)\n     - GPT-5: $1.25/$10 per million tokens\n     - GPT-5-mini: $0.25/$2 per million tokens\n     - Sonnet-4: $5/$20 per million tokens\n     - Opus-4: $25/$100 per million tokens\n     - Gemini-2.5-pro: $3/$12 per million tokens\n     - Grok-4: $5/$15 per million tokens\n   - Calculate based on file size (roughly: file_size_bytes / 4 = tokens)\n   \n   **For single model:**\n   - Say: \"I've prepared the expert consultation (~{tokens} tokens). Model: {model}. You can edit the file to refine your question. Reply 'yes' to send to the expert (estimated cost: ${cost}).\"\n   \n   **For ensemble mode (multiple models):**\n   - Calculate total cost across all models\n   - Say: \"I've prepared an ensemble consultation (~{tokens} tokens) with {models}. Total estimated cost: ${total_cost} ({model1}: ${cost1}, {model2}: ${cost2}, ...). Reply 'yes' to proceed with all models in parallel.\"\n\n6. Execute based on mode:\n\n   **Single Model Mode:**\n   ```bash\n   promptcode expert --prompt-file \"/tmp/expert-consultation-{timestamp}.md\" --model {model} --yes\n   ```\n   \n   **Ensemble Mode (Parallel Execution):**\n   - Use Task tool to run multiple models in parallel\n   - Each task runs the same consultation file with different models\n   - Store each result in separate file: `/tmp/expert-{model}-{timestamp}.txt`\n   - Example for 3 models (run these in PARALLEL using Task tool):\n     ```\n     Task 1: promptcode expert --prompt-file \"/tmp/expert-consultation-{timestamp}.md\" --model o3 --yes > /tmp/expert-o3-{timestamp}.txt\n     Task 2: promptcode expert --prompt-file \"/tmp/expert-consultation-{timestamp}.md\" --model gpt-5 --yes > /tmp/expert-gpt5-{timestamp}.txt  \n     Task 3: promptcode expert --prompt-file \"/tmp/expert-consultation-{timestamp}.md\" --model gemini-2.5-pro --yes > /tmp/expert-gemini-{timestamp}.txt\n     ```\n   - IMPORTANT: Launch all tasks at once for true parallel execution\n   - Wait for all tasks to complete\n   - Note: The --yes flag confirms we have user approval for the cost\n\n7. Handle the response:\n\n   **Single Model Mode:**\n   - If successful: Open response in Cursor (if available) and summarize key insights\n   - If API key missing: Show appropriate setup instructions\n   \n   **Ensemble Mode (Synthesis):**\n   - Read all response text files\n   - Extract key insights from each model's response\n   - Create synthesis report in `/tmp/expert-ensemble-synthesis-{timestamp}.md`:\n   \n   ```markdown\n   # Ensemble Expert Consultation Results\n   \n   ## Question\n   {original_question}\n   \n   ## Expert Responses\n   \n   ### {Model1} - ${actual_cost}, {response_time}s\n   **Key Points:**\n   - {key_point_1}\n   - {key_point_2}\n   - {key_point_3}\n   \n   ### {Model2} - ${actual_cost}, {response_time}s\n   **Key Points:**\n   - {key_point_1}\n   - {key_point_2}\n   - {key_point_3}\n   \n   ## Synthesis\n   \n   **Consensus Points:**\n   - {point_agreed_by_multiple_models}\n   - {another_consensus_point}\n   \n   **Best Comprehensive Answer:** {Model} provided the most thorough analysis, particularly strong on {specific_aspect}\n   \n   **Unique Insights:**\n   - {Model1}: {unique_insight_from_model1}\n   - {Model2}: {unique_insight_from_model2}\n   \n   **Performance Summary:**\n   - Total Cost: ${total_actual_cost}\n   - Total Time: {total_time}s\n   - Best Value: {model_with_best_cost_to_quality_ratio}\n   ```\n   \n   - Open synthesis in Cursor if available\n   - Provide brief summary of which model performed best and why\n\n   **Error Handling:**\n   - If any model fails in ensemble mode, continue with successful ones\n   - Report which models succeeded/failed\n   - If OPENAI_API_KEY missing:\n     ```\n     To use expert consultation, set your OpenAI API key:\n     export OPENAI_API_KEY=sk-...\n     Get your key from: https://platform.openai.com/api-keys\n     ```\n   - For other errors: Report exact error message\n\n## Important:\n- Default to O3 model unless O3-pro explicitly requested or needed for complex reasoning\n- For ensemble mode: limit to maximum 4 models to prevent resource exhaustion\n- Always show cost estimate before sending\n- Keep questions clear and specific\n- Include relevant code context when asking about specific functionality\n- NEVER automatically add --yes without user approval\n- Reasoning effort defaults to 'high' (set in CLI) - no need to specify",
  "promptcode-preset-create.md": "---\nallowed-tools: Bash(promptcode preset create:*), Bash(promptcode preset info:*), Glob(**/*), Grep, Write(.promptcode/presets/*.patterns)\ndescription: Create a promptcode preset from description\n---\n\nCreate a promptcode preset for: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the description to understand what code to capture:\n   - Look for keywords like package names, features, components, integrations\n   - Identify if it's Python, TypeScript, or mixed code\n   - Determine the scope (single package, cross-package feature, etc.)\n\n2. Research the codebase structure:\n   - Use Glob to explore relevant directories\n   - Use Grep to find related files if needed\n   - Identify the main code locations and any related tests/docs\n\n3. Generate a descriptive preset name:\n   - Use kebab-case (e.g., \"auth-system\", \"microlearning-utils\")\n   - Keep it concise but descriptive\n\n4. Create the preset:\n   ```bash\n   promptcode preset create \"{preset_name}\"\n   ```\n   This creates `.promptcode/presets/{preset_name}.patterns`\n\n5. Edit the preset file to add patterns:\n   - Start with a header comment explaining what the preset captures\n   - Add inclusion patterns for the main code\n   - Add patterns for related tests and documentation\n   - Include common exclusion patterns:\n     - `!**/__pycache__/**`\n     - `!**/*.pyc`\n     - `!**/node_modules/**`\n     - `!**/dist/**`\n     - `!**/build/**`\n\n6. Test and report results:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n   Report the file count and estimated tokens.\n\n## Common Pattern Examples:\n- Python package: `python/cogflows-py/packages/{package}/src/**/*.py`\n- TypeScript component: `ts/next/{site}/components/{component}/**/*.{ts,tsx}`\n- Cross-package feature: Multiple specific paths\n- Tests: `python/cogflows-py/packages/{package}/tests/**/*.py`\n- Documentation: `**/{feature}/**/*.md`",
  "promptcode-preset-info.md": "---\nallowed-tools: Bash(promptcode preset info:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*)\ndescription: Show detailed information about a promptcode preset\n---\n\nShow detailed information about promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse the arguments to identify the preset:\n   - If exact preset name provided (e.g., \"functional-framework\"), use it directly\n   - If description provided, infer the best matching preset:\n     - Run `promptcode preset list` to see available presets\n     - Read header comments from preset files in `.promptcode/presets/` if needed\n     - Match based on keywords and context\n     - Choose the most relevant preset\n\n2. Run the promptcode info command with the determined preset name:\n   ```bash\n   promptcode preset info \"{preset_name}\"\n   ```\n\n3. If a preset was inferred from description, explain which preset was chosen and why.\n\nThe output will show:\n- Preset name and path\n- Description from header comments\n- File count and token statistics\n- Pattern details\n- Sample files included\n- Usage instructions",
  "promptcode-preset-list.md": "---\nallowed-tools: Bash(promptcode preset list:*)\ndescription: List all available promptcode presets with pattern counts\n---\n\nList all available promptcode presets.\n\nRun the command:\n```bash\npromptcode preset list\n```\n\nThis will display all available presets with their pattern counts. Use the preset names with other promptcode commands to work with specific code contexts.",
  "promptcode-preset-to-prompt.md": "---\nallowed-tools: Bash(promptcode generate:*), Bash(promptcode preset list:*), Glob(.promptcode/presets/*.patterns), Read(.promptcode/presets/*.patterns:*)\ndescription: Generate AI-ready prompt file from a promptcode preset\n---\n\nGenerate prompt file from promptcode preset: $ARGUMENTS\n\n## Instructions:\n\n1. Parse arguments to understand what the user wants:\n   - Extract preset name or description\n   - Extract output path/filename if specified (e.g., \"to ~/Desktop/analysis.txt\", \"in /tmp/\", \"as myfile.txt\")\n\n2. If inferring from description:\n   - Run `promptcode preset list` to see available presets\n   - Read header comments from `.promptcode/presets/*.patterns` files if needed\n   - Match based on keywords and context\n   - Choose the most relevant preset\n\n3. Determine output path:\n   - Default: `/tmp/promptcode-{preset-name}-{timestamp}.txt` where timestamp is YYYYMMDD-HHMMSS\n   - If user specified just a folder: `{folder}/promptcode-{preset-name}-{timestamp}.txt`\n   - If user specified filename without path: `/tmp/{filename}`\n   - If user specified full path: use exactly as specified\n\n4. Generate the prompt file:\n   ```bash\n   promptcode generate --preset \"{preset_name}\" --output \"{output_path}\"\n   ```\n\n5. Report results:\n   - Which preset was used (especially important if inferred)\n   - Full path to the output file\n   - Token count and number of files included\n   - Suggest next steps (e.g., \"You can now open this file in your editor\")\n\n## Examples of how users might call this:\n- `/promptcode-preset-to-prompt functional-framework`\n- `/promptcode-preset-to-prompt microlearning analysis to ~/Desktop/`\n- `/promptcode-preset-to-prompt the functional code as analysis.txt`"
};

export function getEmbeddedTemplates(): Record<string, string> {
  return EMBEDDED_TEMPLATES;
}

export function hasEmbeddedTemplates(): boolean {
  return Object.keys(EMBEDDED_TEMPLATES).length > 0;
}
